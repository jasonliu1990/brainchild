{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqgjAeDc3fQO",
    "outputId": "afe9820e-1213-47b9-ea7d-8f8ef02fb860"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import catboost as cb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MmPXIGm33kZV"
   },
   "outputs": [],
   "source": [
    "#ccba\n",
    "ccba = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_ccba_full_hashed.csv')\n",
    "#cdtx\n",
    "cdtx = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_cdtx0001_full_hashed.csv')\n",
    "#custinfo\n",
    "custinfo = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_custinfo_full_hashed.csv')\n",
    "#dp\n",
    "dp = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_dp_full_hashed.csv')\n",
    "#remit\n",
    "remit = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_remit1_full_hashed.csv')\n",
    "#train_alert_time\n",
    "train_alert_time = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\train_x_alert_date.csv')\n",
    "#predict_alert_time\n",
    "predict_alert_time = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_x_alert_date.csv')\n",
    "#y\n",
    "y = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\train_y_answer.csv')\n",
    "#案件名單\n",
    "doc = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\預測的案件名單及提交檔案範例.csv')\n",
    "#prev_list set to 0 before submission\n",
    "doc_merged = custinfo.merge(doc, on='alert_key', how='right').merge(predict_alert_time, on='alert_key', how='left')\n",
    "doc_merged = doc_merged[~doc_merged['cust_id'].isnull()]\n",
    "max_doc = doc_merged.groupby('cust_id')['date'].max().reset_index()\n",
    "max_doc.rename(columns={'date':'max_date'},inplace=True)\n",
    "doc_merged = doc_merged.merge(max_doc, on='cust_id', how='left')\n",
    "prev_list = doc_merged[doc_merged['max_date']>doc_merged['date']]['alert_key'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3o4rrFewaSw9"
   },
   "outputs": [],
   "source": [
    "def train_alert_process_func(data, custinfo, predict_alert_time):\n",
    "  alert_data = data[data['cust_id'].isin(custinfo[custinfo['alert_key'].isin(predict_alert_time['alert_key'].tolist())]['cust_id'].tolist())]\n",
    "  train_data = data[~data['cust_id'].isin(custinfo[custinfo['alert_key'].isin(predict_alert_time['alert_key'].tolist())]['cust_id'].tolist())]\n",
    "  train_data['y'] = 0\n",
    "  sar_idx = train_data[train_data['cust_id'].isin(custinfo[custinfo['alert_key'].isin(y[y['sar_flag']==1]['alert_key'].tolist())]['cust_id'].tolist())].index\n",
    "  train_data.loc[sar_idx, 'y'] = 1\n",
    "  return train_data, alert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "q235FTxmizJR"
   },
   "outputs": [],
   "source": [
    "#Data Cleansing\n",
    "#SAR戶\b\b僅留前X天\n",
    "def train_prev_d(x, day):\n",
    "  prev_d = x.groupby('cust_id')['tx_date'].max() - day\n",
    "  prev_d = prev_d.reset_index()\n",
    "  prev_d.rename(columns={'tx_date':'prev_d'}, inplace=True)\n",
    "  x = x.merge(prev_d, on='cust_id', how='left')\n",
    "  x.drop(x[x['tx_date']<x['prev_d']].index,inplace=True)\n",
    "  x.pop('prev_d')\n",
    "  return x\n",
    "#特徵前處理\n",
    "def preprocess(data):\n",
    "  dict1 = {}\n",
    "  idx = 0\n",
    "  num = 0\n",
    "  for i in range(0,395,1):\n",
    "    dict1[i] = str(idx)\n",
    "    num += 1\n",
    "    if num == 7:\n",
    "      idx += 1\n",
    "      num = 0\n",
    "  data['tx_date_group'] = data.tx_date.map(lambda x: dict1[x])\n",
    "  data['session_cust_id'] = data.tx_date_group + data.cust_id\n",
    "  data['date_cust_id'] = data.tx_date.astype(str) + data.cust_id\n",
    "  data['date_time_cust_id'] = data.tx_date.astype(str) + data.tx_date.astype(str) + data.cust_id\n",
    "  data['tx_year'] = 2021\n",
    "  data['tx_date_formal'] = data['tx_date']+1\n",
    "  data.loc[data[data['tx_date_formal']>365].index,'tx_year'] = 2022\n",
    "  data.loc[data[data['tx_date_formal']>365].index, 'tx_date_formal'] = \\\n",
    "  data.loc[data[data['tx_date_formal']>365].index, 'tx_date_formal'] -365\n",
    "  data[\"tx_date_formal\"] = data[\"tx_year\"]*1000 + data[\"tx_date_formal\"]\n",
    "  data['tx_date_formal'] = pd.to_datetime(data['tx_date_formal'], format=\"%Y%j\")\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DsaTx2WdcDcS"
   },
   "outputs": [],
   "source": [
    "def cdtx_feature_func(data):\n",
    "  ##(t-1)d diff\n",
    "  #data['prev_d_diff'] = data['trans_date'] - data.groupby('cust_id')['trans_date'].shift(1)\n",
    "  #data['prev_d_diff'].fillna(15, inplace=True)\n",
    "  #amt\n",
    "  all_amt_sum = (data.groupby('cust_id')['amt'].sum()/data.groupby('cust_id')['amt'].sum().max())\\\n",
    "              .rename('all_amt_sum').reset_index()\n",
    "  data = data.merge(all_amt_sum, on='cust_id', how='left')\n",
    "  date_amt_sum = (data.groupby(['cust_id','date'])['amt'].sum()/data.groupby(['cust_id','date'])['amt'].sum().max())\\\n",
    "              .rename('date_amt_sum').reset_index()\n",
    "  data = data.merge(date_amt_sum, on=['cust_id','date'], how='left')\n",
    "  #txn_cnt\n",
    "  all_txn_cnt = data.groupby('cust_id')['date'].count().rename('cdtx_all_txn_cnt').reset_index()\n",
    "  data = data.merge(all_txn_cnt, on='cust_id', how='left')\n",
    "  data['cdtx_all_txn_cnt'].fillna(0, inplace=True)\n",
    "  date_txn_cnt = data.groupby(['cust_id','date'])['amt'].count().rename('date_txn_cnt').reset_index()\n",
    "  data = data.merge(date_txn_cnt, on=['cust_id','date'], how='left')\n",
    "  #country\n",
    "  all_country_cnt = data.groupby('cust_id')['country'].count().rename('cdtx_all_country_cnt').reset_index()\n",
    "  data = data.merge(all_country_cnt, on='cust_id', how='left')\n",
    "  data['cdtx_all_country_cnt'].fillna(0, inplace=True)\n",
    "  date_country_cnt = data.groupby(['cust_id','date'])['country'].count().rename('date_country_cnt').reset_index()\n",
    "  data = data.merge(date_country_cnt, on=['cust_id','date'], how='left')\n",
    "  #cur_type\n",
    "  all_cur_type_cnt = data.groupby('cust_id')['cur_type'].count().rename('cdtx_all_cur_type_cnt').reset_index()\n",
    "  data = data.merge(all_cur_type_cnt, on='cust_id', how='left')\n",
    "  data['cdtx_all_cur_type_cnt'].fillna(0, inplace=True)\n",
    "  date_cur_type_cnt = data.groupby(['cust_id','date'])['cur_type'].count().rename('date_cur_type_cnt').reset_index()\n",
    "  data = data.merge(date_cur_type_cnt, on=['cust_id','date'], how='left')\n",
    "  #max_min_date_diff\n",
    "  max_min_date_diff = data.groupby(['cust_id']).apply(lambda x: x['date'].max() - x['date'].min()).rename('max_min_date_diff').reset_index()\n",
    "  data = data.merge(max_min_date_diff, on='cust_id', how='left')\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "g7Nxfaet2viO"
   },
   "outputs": [],
   "source": [
    "def remit_feature_func(data):\n",
    "  ##(t-1)d diff\n",
    "  #data['prev_d_diff'] = data['trans_date'] - data.groupby('cust_id')['trans_date'].shift(1)\n",
    "  #data['prev_d_diff'].fillna(15, inplace=True)\n",
    "  #amt\n",
    "  all_amt_sum = (data.groupby('cust_id')['trade_amount_usd'].sum()/data.groupby('cust_id')['trade_amount_usd'].sum().max())\\\n",
    "              .rename('all_amt_sum').reset_index()\n",
    "  data = data.merge(all_amt_sum, on='cust_id', how='left')\n",
    "  time_amt_sum = (data.groupby(['cust_id','trans_date'])['trade_amount_usd'].sum()/data.groupby(['cust_id','trans_date'])['trade_amount_usd'].sum().max())\\\n",
    "              .rename('time_amt_sum').reset_index()\n",
    "  data = data.merge(time_amt_sum, on=['cust_id','trans_date'], how='left')\n",
    "  data['trans_date'] = data['trans_date']/data['trans_date'].max()\n",
    "  #txn_cnt\n",
    "  all_txn_cnt = data.groupby('cust_id')['trans_date'].count().rename('remit_all_txn_cnt').reset_index()\n",
    "  data = data.merge(all_txn_cnt, on='cust_id', how='left')\n",
    "  data['remit_all_txn_cnt'].fillna(0, inplace=True)\n",
    "  date_txn_cnt = data.groupby(['cust_id','trans_date'])['trans_no'].count().rename('date_txn_cnt').reset_index()\n",
    "  data = data.merge(date_txn_cnt, on=['cust_id','trans_date'], how='left')\n",
    "  #max_min_date_diff\n",
    "  max_min_date_diff = data.groupby(['cust_id']).apply(lambda x: x['trans_date'].max() - x['trans_date'].min()).rename('max_min_date_diff').reset_index()\n",
    "  data = data.merge(max_min_date_diff, on='cust_id', how='left')\n",
    "  remit_pivot = data.groupby(['cust_id','trans_date','trans_no'])['trade_amount_usd'].sum().reset_index().pivot_table(index=['cust_id','trans_date'],columns='trans_no',values='trade_amount_usd').reset_index()\n",
    "  remit_pivot.fillna(0, inplace=True)\n",
    "  remit_col = list(remit_pivot.columns)\n",
    "  remit_col.remove('cust_id')\n",
    "  remit_col.remove('trans_date')\n",
    "  remit_pivot['remit_txn_sum'] = remit_pivot[remit_col].sum(axis=1)\n",
    "  for col in remit_col:\n",
    "    remit_pivot[col] = remit_pivot[col]/remit_pivot['remit_txn_sum']\n",
    "  for col in list(set([num for num in range(0,5,1)]) -  set(remit_pivot.columns[2:])):\n",
    "    remit_pivot[col] = 0.0\n",
    "  remit_pivot.rename(columns={0:'remit_trans_no_0',1:'remit_trans_no_1',2:'remit_trans_no_2',3:'remit_trans_no_3',4:'remit_trans_no_4'}, inplace=True)\n",
    "  remit_pivot.fillna(0, inplace=True)\n",
    "  data = data.merge(remit_pivot, on=['cust_id', 'trans_date'], how='left')\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tnmkUyO5ng-R"
   },
   "outputs": [],
   "source": [
    "#session 交易差額比率\n",
    "def dp_feature_func(data):\n",
    "  session_amt_diff = data.groupby(['session_cust_id','debit_credit'])['tx_amt'].sum().reset_index()\n",
    "  session_amt_diff = pd.pivot_table(session_amt_diff, index='session_cust_id', columns='debit_credit', values='tx_amt')\n",
    "  session_amt_diff.fillna(1, inplace=True)\n",
    "  session_amt_diff['session_amt_diff_ratio'] = `````````````\\\n",
    "    abs(session_amt_diff['CR'] - session_amt_diff['DB']) / abs(session_amt_diff['CR'] + session_amt_diff['DB'])\n",
    "  session_amt_diff = session_amt_diff.reset_index()[['session_cust_id','session_amt_diff_ratio']]\n",
    "  data = data.merge(session_amt_diff, on='session_cust_id', how='left')\n",
    "  #當日 交易差額比率\n",
    "  date_amt_diff = data.groupby(['date_cust_id','debit_credit'])['tx_amt'].sum().reset_index()\n",
    "  date_amt_diff = pd.pivot_table(date_amt_diff, index='date_cust_id', columns='debit_credit', values='tx_amt')\n",
    "  date_amt_diff.fillna(1, inplace=True)\n",
    "  date_amt_diff['date_amt_diff_ratio'] = \\\n",
    "  abs(date_amt_diff['CR'] - date_amt_diff['DB']) / abs(date_amt_diff['CR'] + date_amt_diff['DB'])\n",
    "  date_amt_diff = date_amt_diff.reset_index()[['date_cust_id','date_amt_diff_ratio']]\n",
    "  data = data.merge(date_amt_diff, on=['date_cust_id'], how='left')\n",
    "  #當時 交易差額比率\n",
    "  date_time_amt_diff = data.groupby(['date_time_cust_id','debit_credit'])['tx_amt'].sum().reset_index()\n",
    "  date_time_amt_diff = pd.pivot_table(date_time_amt_diff, index='date_time_cust_id', columns='debit_credit', values='tx_amt')\n",
    "  date_time_amt_diff.fillna(1, inplace=True)\n",
    "  date_time_amt_diff['date_time_amt_diff_ratio'] = \\\n",
    "  abs(date_time_amt_diff['CR'] - date_time_amt_diff['DB']) / abs(date_time_amt_diff['CR'] + date_time_amt_diff['DB'])\n",
    "  date_time_amt_diff = date_time_amt_diff.reset_index()[['date_time_cust_id','date_time_amt_diff_ratio']]\n",
    "  data = data.merge(date_time_amt_diff, on=['date_time_cust_id'], how='left')\n",
    "  #當時交易筆數 tx_cnt_date_time\n",
    "  tx_cnt_date_time = data.groupby(['cust_id','tx_date','tx_time'])['debit_credit'].count().reset_index()\n",
    "  tx_cnt_date_time.rename(columns={'debit_credit':'tx_cnt_date_time'}, inplace=True)\n",
    "  data = data.merge(tx_cnt_date_time, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "  #當日交易筆數 tx_cnt_date\n",
    "  tx_cnt_date = data.groupby(['cust_id','tx_date'])['debit_credit'].count().reset_index()\n",
    "  tx_cnt_date.rename(columns={'debit_credit':'tx_cnt_date'}, inplace=True)\n",
    "  data = data.merge(tx_cnt_date, on=['cust_id','tx_date'], how='left')\n",
    "  #當時總分行數 txbranch_day_cnt\n",
    "  txbranch_day_time_cnt = data.groupby(['cust_id','tx_date','tx_time'])['txbranch'].count().reset_index()\n",
    "  txbranch_day_time_cnt.rename(columns={'txbranch':'txbranch_day_time_cnt'}, inplace=True)\n",
    "  data = data.merge(txbranch_day_time_cnt, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "  #單日總分行數 txbranch_day_cnt\n",
    "  txbranch_day_cnt = data.groupby(['cust_id','tx_date'])['txbranch'].count().reset_index()\n",
    "  txbranch_day_cnt.rename(columns={'txbranch':'txbranch_day_cnt'}, inplace=True)\n",
    "  data = data.merge(txbranch_day_cnt, on=['cust_id','tx_date'], how='left')\n",
    "  #當日ATM 佔交易數比例\n",
    "  day_atm_txn_ratio = data.groupby(['cust_id','tx_date'])['ATM'].sum().reset_index()\n",
    "  day_atm_txn_ratio.rename(columns={'ATM':'day_atm_txn_ratio'}, inplace=True)\n",
    "  data = data.merge(day_atm_txn_ratio, on=['cust_id','tx_date'], how='left')\n",
    "  data.day_atm_txn_ratio = data.day_atm_txn_ratio / data.tx_cnt_date\n",
    "  #當時ATM 佔交易數比例\n",
    "  day_time_atm_txn_ratio = data.groupby(['cust_id','tx_date','tx_time'])['ATM'].sum().reset_index()\n",
    "  day_time_atm_txn_ratio.rename(columns={'ATM':'day_time_atm_txn_ratio'}, inplace=True)\n",
    "  data = data.merge(day_time_atm_txn_ratio, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "  data.day_time_atm_txn_ratio = data.day_time_atm_txn_ratio / data.tx_cnt_date_time\n",
    "  #當日跨行 佔交易數比例\n",
    "  day_cross_bank_ratio = data.groupby(['cust_id','tx_date'])['cross_bank'].sum().reset_index()\n",
    "  day_cross_bank_ratio.rename(columns={'cross_bank':'day_cross_bank_ratio'}, inplace=True)\n",
    "  data = data.merge(day_cross_bank_ratio, on=['cust_id','tx_date'], how='left')\n",
    "  data.day_cross_bank_ratio = data.day_cross_bank_ratio / data.tx_cnt_date\n",
    "  #當時跨行 佔交易數比例\n",
    "  day_time_cross_bank_ratio = data.groupby(['cust_id','tx_date','tx_time'])['cross_bank'].sum().reset_index()\n",
    "  day_time_cross_bank_ratio.rename(columns={'cross_bank':'day_time_cross_bank_ratio'}, inplace=True)\n",
    "  data = data.merge(day_time_cross_bank_ratio, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "  data.day_time_cross_bank_ratio = data.day_time_cross_bank_ratio / data.tx_cnt_date_time\n",
    "\n",
    "  ##\n",
    "  #當session交易筆數 tx_cnt_session\n",
    "  tx_cnt_session = data.groupby(['session_cust_id'])['debit_credit'].count().reset_index()\n",
    "  tx_cnt_session.rename(columns={'debit_credit':'tx_cnt_session'}, inplace=True)\n",
    "  data = data.merge(tx_cnt_session, on=['session_cust_id'], how='left')\n",
    "  #當session總分行數 txbranch_session_cnt\n",
    "  txbranch_session_cnt = data.groupby(['session_cust_id'])['txbranch'].count().reset_index()\n",
    "  txbranch_session_cnt.rename(columns={'txbranch':'txbranch_session_cnt'}, inplace=True)\n",
    "  data = data.merge(txbranch_session_cnt, on=['session_cust_id'], how='left')\n",
    "  #當session跨行 佔交易數比例\n",
    "  session_cross_bank_ratio = data.groupby(['session_cust_id'])['cross_bank'].sum().reset_index()\n",
    "  session_cross_bank_ratio.rename(columns={'cross_bank':'session_cross_bank_ratio'}, inplace=True)\n",
    "  data = data.merge(session_cross_bank_ratio, on=['session_cust_id'], how='left')\n",
    "  data.session_cross_bank_ratio = data.session_cross_bank_ratio / data.tx_cnt_date\n",
    "  #當sessionATM 佔交易數比例\n",
    "  session_atm_txn_ratio = data.groupby(['session_cust_id'])['ATM'].sum().reset_index()\n",
    "  session_atm_txn_ratio.rename(columns={'ATM':'session_atm_txn_ratio'}, inplace=True)\n",
    "  data = data.merge(session_atm_txn_ratio, on=['session_cust_id'], how='left')\n",
    "  data.session_atm_txn_ratio = data.session_atm_txn_ratio / data.tx_cnt_date\n",
    "  #time_diff\n",
    "  distinct_date_time = data[['cust_id','tx_date','tx_time']].drop_duplicates().sort_values(['cust_id','tx_date','tx_time']).reset_index(drop=True)\n",
    "  distinct_date_time['date_diff'] = distinct_date_time.groupby('cust_id').apply(lambda x: x['tx_date'] - x['tx_date'].shift(1)).reset_index(drop=True)\n",
    "  distinct_date_time['time_diff'] = distinct_date_time.groupby('cust_id').apply(lambda x: x['tx_time'] - x['tx_time'].shift(1)).reset_index(drop=True)\n",
    "  distinct_date_time.fillna(0, inplace=True)\n",
    "  distinct_date_time['time_diff'] = (distinct_date_time['date_diff']*24) + distinct_date_time['time_diff']\n",
    "  data = data.merge(distinct_date_time[['cust_id', 'tx_date', 'tx_time', 'time_diff']], on=['cust_id', 'tx_date', 'tx_time'], how='left')\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "id": "kbibYxDPdbzv"
   },
   "outputs": [],
   "source": [
    "def model_training_1(data, columns):\n",
    "  #逐筆交易處理\n",
    "  Y = data['y']\n",
    "  X = data[columns[1:-1]]\n",
    "  test_size = 0.2\n",
    "  seed = 42\n",
    "#   X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)#, stratify = Y)\n",
    "  model = XGBClassifier(\n",
    "                    base_score= 0.5, \n",
    "                booster= 'gbtree', \n",
    "                colsample_bylevel= 1, \n",
    "                colsample_bynode= 1, \n",
    "                colsample_bytree= 1, \n",
    "                gamma= 0, \n",
    "                learning_rate= 0.03,\n",
    "                max_delta_step= 0, \n",
    "                max_depth= 2, \n",
    "                min_child_weight= 1, \n",
    "#                 missing= None, \n",
    "                n_estimators= 90, \n",
    "                nthread= 16, \n",
    "                objective= 'binary:logistic', \n",
    "                reg_alpha= 0, \n",
    "                reg_lambda= 1, \n",
    "                scale_pos_weight= 1, \n",
    "                seed= 0, \n",
    "                subsample= 0.9,\n",
    "                verbosity= 1\n",
    "  )\n",
    "  model.fit(X, Y)\n",
    "#   y_pred = model.predict(X_test)\n",
    "#   predictions = [round(value) for value in y_pred]\n",
    "  # evaluate predictions\n",
    "#   accuracy = accuracy_score(y_test, predictions)\n",
    "#   print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "#   f1 = f1_score(y_test, predictions)\n",
    "#   print(\"F1 Scoure: %.2f%%\" % (f1 * 100.0))\n",
    "#   print(precision_score(y_test, predictions))\n",
    "#   print(recall_score(y_test, predictions))\n",
    "  feature_importance = pd.DataFrame({'columns':list(X.columns),'score':model.feature_importances_})\n",
    "  return X, model, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "id": "anmklWd_dqpY"
   },
   "outputs": [],
   "source": [
    "#彙整model training 1偵測結果\n",
    "#predict_proba以0.05機率區間為一個欄位判斷分佈\n",
    "def mapping(proba):\n",
    "    if proba <= 0.1:\n",
    "        return 1\n",
    "    elif 0.1 < proba <= 0.2:\n",
    "        return 2\n",
    "    elif 0.2 < proba <= 0.3:\n",
    "        return 3\n",
    "    elif 0.3 < proba <= 0.4:\n",
    "        return 4\n",
    "    elif 0.4 < proba <= 0.5:\n",
    "        return 5\n",
    "    elif 0.5 < proba <= 0.6:\n",
    "        return 6\n",
    "    elif 0.6 < proba <= 0.7:\n",
    "        return 7\n",
    "    elif 0.7 < proba <= 0.8:\n",
    "        return 8\n",
    "    elif 0.8 < proba <= 0.9:\n",
    "        return 9\n",
    "    elif 0.9 < proba <= 1:\n",
    "        return 10\n",
    "def debit_credit_ratio_func(data):\n",
    "  #id\n",
    "  debit_credit_ratio = data.groupby(['cust_id'])['debit_credit'].value_counts().rename('debit_credit_ratio').reset_index()\n",
    "  debit_credit_ratio = debit_credit_ratio.pivot_table(values='debit_credit_ratio', index=['cust_id'], columns='debit_credit')\n",
    "  debit_credit_ratio.fillna(0, inplace=True)\n",
    "  debit_credit_ratio['debit_credit_ratio'] = debit_credit_ratio['DB']/debit_credit_ratio.sum(axis=1)\n",
    "  debit_credit_ratio = debit_credit_ratio.reset_index()[['cust_id','debit_credit_ratio']]\n",
    "  data = data.merge(debit_credit_ratio, on=['cust_id'], how='left')\n",
    "  return debit_credit_ratio\n",
    "def all_txn_cnt(data):\n",
    "  all_txn_cnt = data.groupby('cust_id')['tx_date'].count().rename('all_txn_cnt').reset_index()\n",
    "  data = data.merge(all_txn_cnt, on='cust_id', how='left')\n",
    "  return all_txn_cnt\n",
    "def result_preprocess_func(data, X, model):\n",
    "  data_db_cr_ratio = debit_credit_ratio_func(data)\n",
    "  data_all_txn_cnt = all_txn_cnt(data)\n",
    "  data['proba'] = model.predict_proba(X)[:,1]\n",
    "  result = data[['cust_id','proba','y']]\n",
    "  result['level'] = result[\"proba\"].map(mapping)\n",
    "  result = result[['cust_id', 'level', 'y']]\n",
    "  result = result.groupby(['cust_id','level']).count().reset_index().pivot_table(index='cust_id', columns='level', values='y')\n",
    "  result.fillna(0, inplace=True)\n",
    "  result = result.div(result.sum(axis=1), axis=0).reset_index()\n",
    "  for col in list(set([num for num in range(1,11,1)]) -  set(result.columns[1:])):\n",
    "    result[col] = 0.0\n",
    "  result = result[['cust_id', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
    "  result = result.merge(data[['cust_id','y']].drop_duplicates(), on='cust_id', how='left')\n",
    "  result = result.merge(data_db_cr_ratio, on='cust_id', how='left')\n",
    "  result = result.merge(data_all_txn_cnt, on='cust_id', how='left')\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "id": "-Tv2I5j8du3O"
   },
   "outputs": [],
   "source": [
    "#model training 2\n",
    "#歸戶判斷是否報SAR\n",
    "def model_training_2(result):\n",
    "  result_col = list(result.columns)\n",
    "  result_col.remove('cust_id')\n",
    "  result_col.remove('y')\n",
    "  test_size = 0.2\n",
    "  seed = 42\n",
    "#   X_train, X_test, y_train, y_test = train_test_split(result[result_col], result['y'], test_size=test_size, random_state=seed)#, stratify = result['y'])\n",
    "  model = cb.CatBoostClassifier(\n",
    "                            learning_rate = 0.3\n",
    "                             , max_depth=2\n",
    "                             , reg_lambda=1\n",
    "                             , n_estimators=100\n",
    "#                              , reg_alpha=0.01\n",
    "                             , subsample = 1\n",
    "  )\n",
    "  model.fit(result[result_col], result['y'])\n",
    "#   y_pred = model.predict(X_test)\n",
    "#   predictions = [round(value) for value in y_pred]\n",
    "#   # evaluate predictions\n",
    "#   accuracy = accuracy_score(y_test, predictions)\n",
    "#   print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "#   f1 = f1_score(y_test, predictions)\n",
    "#   print(\"F1 Scoure: %.2f%%\" % (f1 * 100.0))\n",
    "#   print(precision_score(y_test, predictions))\n",
    "#   print(recall_score(y_test, predictions))\n",
    "  feature_importance = pd.DataFrame({'columns':list(result[result_col].columns),'score':model.feature_importances_})\n",
    "  return model, result_col, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "id": "E41cMxVIjkaR"
   },
   "outputs": [],
   "source": [
    "def alert_output(alert, model_1, model_2, alert_col, result_col, doc):\n",
    "  data_db_cr_ratio = debit_credit_ratio_func(alert)\n",
    "  data_all_txn_cnt = all_txn_cnt(alert)\n",
    "\n",
    "  #Remit Submission\n",
    "  alert_data = alert\n",
    "  alert_data['proba'] = model_1.predict_proba(alert[alert_col])[:,1]\n",
    "  alert_result = alert_data[['cust_id','proba']]\n",
    "  alert_result['level'] = alert_result[\"proba\"].map(mapping)\n",
    "  alert_result = alert_result[['cust_id', 'level']]\n",
    "  alert_result['cnt'] = 1\n",
    "  alert_result = alert_result.groupby(['cust_id','level'])['cnt'].count().reset_index().pivot_table(index='cust_id', columns='level', values='cnt')\n",
    "  alert_result.fillna(0, inplace=True)\n",
    "  alert_result = alert_result.div(alert_result.sum(axis=1), axis=0)\n",
    "  alert_result = alert_result.reset_index()\n",
    "  for col in list(set([num for num in range(1,11,1)]) -  set(alert_result.columns[1:])):\n",
    "    alert_result[col] = 0.0\n",
    "  alert_result = alert_result.merge(data_db_cr_ratio, on='cust_id', how='left')\n",
    "  alert_result = alert_result.merge(data_all_txn_cnt, on='cust_id', how='left')\n",
    "  alert_pred = model_2.predict_proba(alert_result[result_col])\n",
    "  # evaluate predictions\n",
    "  alert_result = alert_result[['cust_id']]\n",
    "  alert_result['probability'] = alert_pred[:,1]\n",
    "\n",
    "  final = predict_alert_time.merge(custinfo[['alert_key', 'cust_id']].merge(alert_result, on='cust_id'), on='alert_key', how='left')[['alert_key', 'probability']]\n",
    "  doc = doc[['alert_key']]\n",
    "  final = doc.merge(final, on='alert_key', how='left')\n",
    "  final.fillna(0,inplace=True)\n",
    "  final.loc[final[final['alert_key'].isin(prev_list)].index,'probability']=0\n",
    "  return final, alert_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "id": "u0_a1IO05ZWZ"
   },
   "outputs": [],
   "source": [
    "def prev_7d_feature_func(data):\n",
    "  #last 7 days processing\n",
    "  data_distinct = data[['cust_id', 'tx_date_formal','tx_date']].drop_duplicates().reset_index(drop=True)\n",
    "  cross_bank_sum = data.groupby(['cust_id','tx_date_formal'])['cross_bank'].sum().rename('cross_bank_sum').reset_index()\n",
    "  prev_7d_data = data_distinct.merge(cross_bank_sum, on=['cust_id','tx_date_formal'], how='left')\n",
    "  dbcr_amt_sum = data.groupby(['cust_id','tx_date_formal','debit_credit'])['tx_amt'].sum().rename('debit_credit_sum').reset_index()\n",
    "  dbcr_amt_sum = pd.pivot_table(dbcr_amt_sum, index=['cust_id','tx_date_formal'], columns='debit_credit', values='debit_credit_sum')\n",
    "  prev_7d_data = prev_7d_data.merge(dbcr_amt_sum, on=['cust_id','tx_date_formal'], how='left')\n",
    "  tx_date_sum = data.groupby(['cust_id','tx_date_formal'])['tx_amt'].sum().rename('tx_amt_sum')\n",
    "  prev_7d_data = prev_7d_data.merge(tx_date_sum, on=['cust_id','tx_date_formal'], how='left')\n",
    "  tx_cnt = data.groupby(['cust_id','tx_date_formal'])['tx_amt'].count().rename('tx_cnt').reset_index()\n",
    "  prev_7d_data = prev_7d_data.merge(tx_cnt, on=['cust_id','tx_date_formal'], how='left')\n",
    "  #當日ATM 佔交易數比例\n",
    "  day_atm_cnt = data.groupby(['cust_id','tx_date_formal'])['ATM'].sum().rename('day_atm_cnt').reset_index()\n",
    "  prev_7d_data = prev_7d_data.merge(day_atm_cnt, on=['cust_id','tx_date_formal'], how='left')\n",
    "  txbranch_day_cnt = data.groupby(['cust_id','tx_date_formal'])['txbranch'].count().rename('txbranch_day_cnt').reset_index()\n",
    "  prev_7d_data = prev_7d_data.merge(txbranch_day_cnt, on=['cust_id','tx_date_formal'], how='left')\n",
    "  prev_7d_data.fillna(0, inplace=True)\n",
    "  prev_7d_data = prev_7d_data.sort_values(['cust_id','tx_date_formal'])\n",
    "  prev_7d_data = prev_7d_data.set_index('tx_date_formal')\n",
    "  #feature engineering\n",
    "  prev_7d_cross_bank_sum = prev_7d_data.groupby('cust_id')['cross_bank_sum'].rolling(window='7D').sum().rename('prev_7d_cross_bank_sum').reset_index()\n",
    "  prev_7d_CR_sum = prev_7d_data.groupby('cust_id')['CR'].rolling(window='7D').sum().rename('prev_7d_CR_sum').reset_index()\n",
    "  prev_7d_DB_sum = prev_7d_data.groupby('cust_id')['DB'].rolling(window='7D').sum().rename('prev_7d_DB_sum').reset_index()\n",
    "  prev_7d_tx_cnt = prev_7d_data.groupby('cust_id')['tx_cnt'].rolling(window='7D').sum().rename('prev_7d_tx_cnt').reset_index()\n",
    "  prev_7d_txbranch_cnt = prev_7d_data.groupby('cust_id')['txbranch_day_cnt'].rolling(window='7D').sum().rename('prev_7d_txbranch_cnt').reset_index()\n",
    "  prev_7d_atm_cnt = prev_7d_data.groupby('cust_id')['day_atm_cnt'].rolling(window='7D').sum().rename('prev_7d_atm_cnt').reset_index()\n",
    "\n",
    "  data_distinct = data_distinct.merge(prev_7d_CR_sum, on=['cust_id','tx_date_formal']).merge(prev_7d_DB_sum, on=['cust_id','tx_date_formal'])\\\n",
    "    .merge(prev_7d_tx_cnt, on=['cust_id','tx_date_formal']).merge(prev_7d_txbranch_cnt, on=['cust_id','tx_date_formal']).merge(prev_7d_atm_cnt, on=['cust_id','tx_date_formal'])\\\n",
    "    .merge(prev_7d_cross_bank_sum, on=['cust_id','tx_date_formal'])\n",
    "\n",
    "  data_distinct['prev_7d_txbranch_ratio'] = data_distinct.prev_7d_txbranch_cnt / data_distinct.prev_7d_tx_cnt\n",
    "  data_distinct['prev_7d_atm_ratio'] = data_distinct.prev_7d_atm_cnt / data_distinct.prev_7d_tx_cnt\n",
    "  data_distinct['prev_7d_cross_bank_ratio'] = data_distinct.prev_7d_cross_bank_sum / data_distinct.prev_7d_tx_cnt\n",
    "  data_distinct['prev7d_amt_diff_ratio'] = \\\n",
    "    abs(data_distinct['prev_7d_CR_sum'] - data_distinct['prev_7d_DB_sum']) / abs(data_distinct['prev_7d_CR_sum'] + data_distinct['prev_7d_DB_sum'])\n",
    "  data_distinct = data_distinct[['cust_id','tx_date','prev_7d_tx_cnt','prev7d_amt_diff_ratio','prev_7d_txbranch_ratio','prev_7d_atm_cnt','prev_7d_atm_ratio',\n",
    "                                 'prev_7d_txbranch_cnt','prev_7d_cross_bank_ratio','prev_7d_cross_bank_sum']]\n",
    "  data = data.merge(data_distinct, on=['cust_id', 'tx_date'], how='left')\n",
    "  return data, data_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "FUdbYkG8o43l",
    "outputId": "f62944ec-42d6-491f-974d-87d45c1d3d94",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1763: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4773857\ttotal: 497us\tremaining: 49.3ms\n",
      "1:\tlearn: 0.3604230\ttotal: 983us\tremaining: 48.2ms\n",
      "2:\tlearn: 0.2875929\ttotal: 1.4ms\tremaining: 45.2ms\n",
      "3:\tlearn: 0.2421407\ttotal: 1.86ms\tremaining: 44.6ms\n",
      "4:\tlearn: 0.2125564\ttotal: 2.3ms\tremaining: 43.8ms\n",
      "5:\tlearn: 0.1922743\ttotal: 2.73ms\tremaining: 42.8ms\n",
      "6:\tlearn: 0.1791567\ttotal: 3.22ms\tremaining: 42.8ms\n",
      "7:\tlearn: 0.1691834\ttotal: 3.71ms\tremaining: 42.6ms\n",
      "8:\tlearn: 0.1635637\ttotal: 4.19ms\tremaining: 42.3ms\n",
      "9:\tlearn: 0.1593157\ttotal: 4.68ms\tremaining: 42.1ms\n",
      "10:\tlearn: 0.1569757\ttotal: 5.13ms\tremaining: 41.5ms\n",
      "11:\tlearn: 0.1549644\ttotal: 5.58ms\tremaining: 40.9ms\n",
      "12:\tlearn: 0.1540861\ttotal: 6.01ms\tremaining: 40.2ms\n",
      "13:\tlearn: 0.1528552\ttotal: 6.46ms\tremaining: 39.7ms\n",
      "14:\tlearn: 0.1521202\ttotal: 6.88ms\tremaining: 39ms\n",
      "15:\tlearn: 0.1515485\ttotal: 7.36ms\tremaining: 38.6ms\n",
      "16:\tlearn: 0.1513092\ttotal: 7.77ms\tremaining: 37.9ms\n",
      "17:\tlearn: 0.1507208\ttotal: 8.27ms\tremaining: 37.7ms\n",
      "18:\tlearn: 0.1506399\ttotal: 8.68ms\tremaining: 37ms\n",
      "19:\tlearn: 0.1503269\ttotal: 9.12ms\tremaining: 36.5ms\n",
      "20:\tlearn: 0.1498498\ttotal: 9.56ms\tremaining: 36ms\n",
      "21:\tlearn: 0.1495597\ttotal: 9.98ms\tremaining: 35.4ms\n",
      "22:\tlearn: 0.1494032\ttotal: 10.4ms\tremaining: 34.9ms\n",
      "23:\tlearn: 0.1493619\ttotal: 10.8ms\tremaining: 34.2ms\n",
      "24:\tlearn: 0.1493474\ttotal: 11.2ms\tremaining: 33.6ms\n",
      "25:\tlearn: 0.1493339\ttotal: 11.6ms\tremaining: 33.1ms\n",
      "26:\tlearn: 0.1492836\ttotal: 12ms\tremaining: 32.5ms\n",
      "27:\tlearn: 0.1492216\ttotal: 12.5ms\tremaining: 32.1ms\n",
      "28:\tlearn: 0.1490709\ttotal: 13ms\tremaining: 31.7ms\n",
      "29:\tlearn: 0.1488887\ttotal: 13.4ms\tremaining: 31.3ms\n",
      "30:\tlearn: 0.1487973\ttotal: 13.8ms\tremaining: 30.7ms\n",
      "31:\tlearn: 0.1487406\ttotal: 14.2ms\tremaining: 30.2ms\n",
      "32:\tlearn: 0.1486977\ttotal: 14.6ms\tremaining: 29.7ms\n",
      "33:\tlearn: 0.1486493\ttotal: 15.1ms\tremaining: 29.2ms\n",
      "34:\tlearn: 0.1484349\ttotal: 15.5ms\tremaining: 28.7ms\n",
      "35:\tlearn: 0.1482151\ttotal: 15.9ms\tremaining: 28.3ms\n",
      "36:\tlearn: 0.1481003\ttotal: 16.3ms\tremaining: 27.8ms\n",
      "37:\tlearn: 0.1479352\ttotal: 16.8ms\tremaining: 27.4ms\n",
      "38:\tlearn: 0.1477504\ttotal: 17.2ms\tremaining: 26.9ms\n",
      "39:\tlearn: 0.1476737\ttotal: 17.6ms\tremaining: 26.4ms\n",
      "40:\tlearn: 0.1471808\ttotal: 18ms\tremaining: 26ms\n",
      "41:\tlearn: 0.1469916\ttotal: 18.4ms\tremaining: 25.5ms\n",
      "42:\tlearn: 0.1468285\ttotal: 18.9ms\tremaining: 25ms\n",
      "43:\tlearn: 0.1463365\ttotal: 19.4ms\tremaining: 24.7ms\n",
      "44:\tlearn: 0.1462946\ttotal: 19.8ms\tremaining: 24.2ms\n",
      "45:\tlearn: 0.1460850\ttotal: 20.2ms\tremaining: 23.8ms\n",
      "46:\tlearn: 0.1459518\ttotal: 20.6ms\tremaining: 23.2ms\n",
      "47:\tlearn: 0.1455027\ttotal: 21ms\tremaining: 22.8ms\n",
      "48:\tlearn: 0.1454516\ttotal: 21.5ms\tremaining: 22.4ms\n",
      "49:\tlearn: 0.1449311\ttotal: 22ms\tremaining: 22ms\n",
      "50:\tlearn: 0.1448699\ttotal: 22.4ms\tremaining: 21.5ms\n",
      "51:\tlearn: 0.1447891\ttotal: 22.9ms\tremaining: 21.1ms\n",
      "52:\tlearn: 0.1447566\ttotal: 23.3ms\tremaining: 20.7ms\n",
      "53:\tlearn: 0.1445191\ttotal: 23.7ms\tremaining: 20.2ms\n",
      "54:\tlearn: 0.1444873\ttotal: 24.2ms\tremaining: 19.8ms\n",
      "55:\tlearn: 0.1442461\ttotal: 24.6ms\tremaining: 19.4ms\n",
      "56:\tlearn: 0.1441581\ttotal: 25ms\tremaining: 18.9ms\n",
      "57:\tlearn: 0.1441286\ttotal: 25.5ms\tremaining: 18.4ms\n",
      "58:\tlearn: 0.1440683\ttotal: 25.9ms\tremaining: 18ms\n",
      "59:\tlearn: 0.1440350\ttotal: 26.3ms\tremaining: 17.5ms\n",
      "60:\tlearn: 0.1439791\ttotal: 26.7ms\tremaining: 17.1ms\n",
      "61:\tlearn: 0.1439288\ttotal: 27.1ms\tremaining: 16.6ms\n",
      "62:\tlearn: 0.1439060\ttotal: 27.5ms\tremaining: 16.2ms\n",
      "63:\tlearn: 0.1438133\ttotal: 27.9ms\tremaining: 15.7ms\n",
      "64:\tlearn: 0.1437843\ttotal: 28.3ms\tremaining: 15.3ms\n",
      "65:\tlearn: 0.1434810\ttotal: 28.8ms\tremaining: 14.8ms\n",
      "66:\tlearn: 0.1433305\ttotal: 29.2ms\tremaining: 14.4ms\n",
      "67:\tlearn: 0.1432585\ttotal: 29.6ms\tremaining: 13.9ms\n",
      "68:\tlearn: 0.1428570\ttotal: 30ms\tremaining: 13.5ms\n",
      "69:\tlearn: 0.1428329\ttotal: 30.5ms\tremaining: 13.1ms\n",
      "70:\tlearn: 0.1427848\ttotal: 30.9ms\tremaining: 12.6ms\n",
      "71:\tlearn: 0.1427382\ttotal: 31.3ms\tremaining: 12.2ms\n",
      "72:\tlearn: 0.1427162\ttotal: 31.7ms\tremaining: 11.7ms\n",
      "73:\tlearn: 0.1426434\ttotal: 32.1ms\tremaining: 11.3ms\n",
      "74:\tlearn: 0.1426050\ttotal: 32.5ms\tremaining: 10.8ms\n",
      "75:\tlearn: 0.1425102\ttotal: 32.9ms\tremaining: 10.4ms\n",
      "76:\tlearn: 0.1423321\ttotal: 33.3ms\tremaining: 9.96ms\n",
      "77:\tlearn: 0.1423112\ttotal: 33.8ms\tremaining: 9.53ms\n",
      "78:\tlearn: 0.1421656\ttotal: 34.2ms\tremaining: 9.09ms\n",
      "79:\tlearn: 0.1421045\ttotal: 34.6ms\tremaining: 8.64ms\n",
      "80:\tlearn: 0.1420877\ttotal: 35ms\tremaining: 8.21ms\n",
      "81:\tlearn: 0.1418857\ttotal: 35.4ms\tremaining: 7.77ms\n",
      "82:\tlearn: 0.1417131\ttotal: 35.8ms\tremaining: 7.34ms\n",
      "83:\tlearn: 0.1416722\ttotal: 36.2ms\tremaining: 6.89ms\n",
      "84:\tlearn: 0.1413726\ttotal: 36.6ms\tremaining: 6.46ms\n",
      "85:\tlearn: 0.1411612\ttotal: 37ms\tremaining: 6.02ms\n",
      "86:\tlearn: 0.1411431\ttotal: 37.4ms\tremaining: 5.59ms\n",
      "87:\tlearn: 0.1410590\ttotal: 37.8ms\tremaining: 5.16ms\n",
      "88:\tlearn: 0.1410222\ttotal: 38.3ms\tremaining: 4.73ms\n",
      "89:\tlearn: 0.1408957\ttotal: 38.7ms\tremaining: 4.3ms\n",
      "90:\tlearn: 0.1407241\ttotal: 39.1ms\tremaining: 3.87ms\n",
      "91:\tlearn: 0.1406222\ttotal: 39.5ms\tremaining: 3.44ms\n",
      "92:\tlearn: 0.1404580\ttotal: 40ms\tremaining: 3.01ms\n",
      "93:\tlearn: 0.1404246\ttotal: 40.4ms\tremaining: 2.58ms\n",
      "94:\tlearn: 0.1404079\ttotal: 40.9ms\tremaining: 2.15ms\n",
      "95:\tlearn: 0.1402008\ttotal: 41.3ms\tremaining: 1.72ms\n",
      "96:\tlearn: 0.1400590\ttotal: 41.7ms\tremaining: 1.29ms\n",
      "97:\tlearn: 0.1399773\ttotal: 42.2ms\tremaining: 861us\n",
      "98:\tlearn: 0.1398856\ttotal: 42.6ms\tremaining: 430us\n",
      "99:\tlearn: 0.1397471\ttotal: 43.1ms\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alert_key</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>357307</td>\n",
       "      <td>0.020872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>376329</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>373644</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>357668</td>\n",
       "      <td>0.055414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>354443</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>364485</td>\n",
       "      <td>0.020688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>363155</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>368710</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>358067</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>372119</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3850 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      alert_key  probability\n",
       "0        357307     0.020872\n",
       "1        376329     0.000000\n",
       "2        373644     0.000000\n",
       "3        357668     0.055414\n",
       "4        354443     0.000000\n",
       "...         ...          ...\n",
       "3845     364485     0.020688\n",
       "3846     363155     0.000000\n",
       "3847     368710     0.000000\n",
       "3848     358067     0.000000\n",
       "3849     372119     0.000000\n",
       "\n",
       "[3850 rows x 2 columns]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dp zone\n",
    "train_dp, alert_dp = train_alert_process_func(dp, custinfo, predict_alert_time)\n",
    "train_dp = train_prev_d(train_dp, 30)\n",
    "alert_dp = train_prev_d(alert_dp, 30)\n",
    "train_dp = preprocess(train_dp)\n",
    "alert_dp = preprocess(alert_dp)\n",
    "##\n",
    "\n",
    "train_dp, train_dp_distinct = prev_7d_feature_func(train_dp)\n",
    "alert_dp, alert_dp_distinct = prev_7d_feature_func(alert_dp)\n",
    "##\n",
    "train_dp = dp_feature_func(train_dp)\n",
    "alert_dp = dp_feature_func(alert_dp)\n",
    "\n",
    "dp_col = ['cust_id','session_amt_diff_ratio', \n",
    "          #,'txbranch_session_cnt','session_atm_txn_ratio','session_cross_bank_ratio',\n",
    "          'date_time_amt_diff_ratio','tx_cnt_date_time','txbranch_day_time_cnt', 'day_time_atm_txn_ratio','day_time_cross_bank_ratio',\n",
    "          'date_amt_diff_ratio','tx_cnt_date','txbranch_day_cnt','day_atm_txn_ratio','day_cross_bank_ratio',#(version : original)\n",
    "          'time_diff',#(version : date_diff_30)\n",
    "#            'tx_cnt_session',#(version : tx_cnt_session)\n",
    "          #'prev7d_amt_diff_ratio',\n",
    "          #'prev_7d_tx_cnt','prev_7d_txbranch_ratio',#'prev_7d_atm_cnt','prev_7d_atm_ratio',\n",
    "          #'prev_7d_txbranch_cnt',#'prev_7d_cross_bank_sum',#'prev_7d_cross_bank_ratio',\n",
    "          'y']\n",
    "dp_X, dp_model_1, dp_feature_importance = model_training_1(train_dp, dp_col)\n",
    "dp_result = result_preprocess_func(train_dp, dp_X, dp_model_1)\n",
    "dp_model_2, dp_result_col, dp_feature_importance2 = model_training_2(dp_result)\n",
    "dp_col.remove('y')\n",
    "dp_col.remove('cust_id')\n",
    "dp_final, alert_dp_result = alert_output(alert_dp, dp_model_1, dp_model_2, dp_col, dp_result_col, doc)\n",
    "dp_final\n",
    "#F1 Scoure: 66.66%\n",
    "#0.9980544747081712\n",
    "#0.5003657644476956\n",
    "#F1 Scoure: 28.57%\n",
    "#0.875\n",
    "#0.17073170731707318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "tW5_zPa6bg6h",
    "outputId": "bcf83957-7b15-4e46-9555-02adea4c094d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>session_amt_diff_ratio</td>\n",
       "      <td>0.077227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date_time_amt_diff_ratio</td>\n",
       "      <td>0.111927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tx_cnt_date_time</td>\n",
       "      <td>0.129007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txbranch_day_time_cnt</td>\n",
       "      <td>0.006482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day_time_atm_txn_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day_time_cross_bank_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>date_amt_diff_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tx_cnt_date</td>\n",
       "      <td>0.532249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>txbranch_day_cnt</td>\n",
       "      <td>0.092074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>day_atm_txn_ratio</td>\n",
       "      <td>0.051034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>day_cross_bank_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>time_diff</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      columns     score\n",
       "0      session_amt_diff_ratio  0.077227\n",
       "1    date_time_amt_diff_ratio  0.111927\n",
       "2            tx_cnt_date_time  0.129007\n",
       "3       txbranch_day_time_cnt  0.006482\n",
       "4      day_time_atm_txn_ratio  0.000000\n",
       "5   day_time_cross_bank_ratio  0.000000\n",
       "6         date_amt_diff_ratio  0.000000\n",
       "7                 tx_cnt_date  0.532249\n",
       "8            txbranch_day_cnt  0.092074\n",
       "9           day_atm_txn_ratio  0.051034\n",
       "10       day_cross_bank_ratio  0.000000\n",
       "11                  time_diff  0.000000"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>18.905216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>11.006715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.016895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>debit_credit_ratio</td>\n",
       "      <td>21.288628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>all_txn_cnt</td>\n",
       "      <td>48.782545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               columns      score\n",
       "0                    1  18.905216\n",
       "1                    2  11.006715\n",
       "2                    3   0.000000\n",
       "3                    4   0.000000\n",
       "4                    5   0.000000\n",
       "5                    6   0.000000\n",
       "6                    7   0.000000\n",
       "7                    8   0.000000\n",
       "8                    9   0.000000\n",
       "9                   10   0.016895\n",
       "10  debit_credit_ratio  21.288628\n",
       "11         all_txn_cnt  48.782545"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_feature_importance2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nan_mode': 'Min',\n",
       " 'eval_metric': 'Logloss',\n",
       " 'iterations': 100,\n",
       " 'sampling_frequency': 'PerTree',\n",
       " 'leaf_estimation_method': 'Newton',\n",
       " 'grow_policy': 'SymmetricTree',\n",
       " 'penalties_coefficient': 1,\n",
       " 'boosting_type': 'Plain',\n",
       " 'model_shrink_mode': 'Constant',\n",
       " 'feature_border_type': 'GreedyLogSum',\n",
       " 'bayesian_matrix_reg': 0.10000000149011612,\n",
       " 'eval_fraction': 0,\n",
       " 'force_unit_auto_pair_weights': False,\n",
       " 'l2_leaf_reg': 1,\n",
       " 'random_strength': 1,\n",
       " 'rsm': 1,\n",
       " 'boost_from_average': False,\n",
       " 'model_size_reg': 0.5,\n",
       " 'pool_metainfo_options': {'tags': {}},\n",
       " 'subsample': 1,\n",
       " 'use_best_model': False,\n",
       " 'class_names': [0, 1],\n",
       " 'random_seed': 0,\n",
       " 'depth': 2,\n",
       " 'posterior_sampling': False,\n",
       " 'border_count': 254,\n",
       " 'classes_count': 0,\n",
       " 'auto_class_weights': 'None',\n",
       " 'sparse_features_conflict_fraction': 0,\n",
       " 'leaf_estimation_backtracking': 'AnyImprovement',\n",
       " 'best_model_min_trees': 1,\n",
       " 'model_shrink_rate': 0,\n",
       " 'min_data_in_leaf': 1,\n",
       " 'loss_function': 'Logloss',\n",
       " 'learning_rate': 0.30000001192092896,\n",
       " 'score_function': 'Cosine',\n",
       " 'task_type': 'CPU',\n",
       " 'leaf_estimation_iterations': 1,\n",
       " 'bootstrap_type': 'MVS',\n",
       " 'max_leaves': 4}"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_model_2.get_all_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "id": "cGvCK_odkTnw"
   },
   "outputs": [],
   "source": [
    "dp_final.to_csv('0.063.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
