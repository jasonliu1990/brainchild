{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqgjAeDc3fQO",
    "outputId": "afe9820e-1213-47b9-ea7d-8f8ef02fb860"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "import catboost as cb\n",
    "import ngboost as nb\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "MmPXIGm33kZV"
   },
   "outputs": [],
   "source": [
    "#ccba\n",
    "ccba = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_ccba_full_hashed.csv')\n",
    "#cdtx\n",
    "cdtx = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_cdtx0001_full_hashed.csv')\n",
    "#custinfo\n",
    "custinfo = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_custinfo_full_hashed.csv')\n",
    "#dp\n",
    "dp = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_dp_full_hashed.csv')\n",
    "#remit\n",
    "remit = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_remit1_full_hashed.csv')\n",
    "#train_alert_time\n",
    "train_alert_time = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\train_x_alert_date.csv')\n",
    "#predict_alert_time\n",
    "predict_alert_time = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_x_alert_date.csv')\n",
    "#y\n",
    "y = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\train_y_answer.csv')\n",
    "#案件名單\n",
    "doc = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\預測的案件名單及提交檔案範例.csv')\n",
    "#prev_list set to 0 before submission\n",
    "doc_merged = custinfo.merge(doc, on='alert_key', how='right').merge(predict_alert_time, on='alert_key', how='left')\n",
    "doc_merged = doc_merged[~doc_merged['cust_id'].isnull()]\n",
    "max_doc = doc_merged.groupby('cust_id')['date'].max().reset_index()\n",
    "max_doc.rename(columns={'date':'max_date'},inplace=True)\n",
    "doc_merged = doc_merged.merge(max_doc, on='cust_id', how='left')\n",
    "prev_list = doc_merged[doc_merged['max_date']>doc_merged['date']]['alert_key'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "3o4rrFewaSw9"
   },
   "outputs": [],
   "source": [
    "def train_alert_process_func(data, custinfo, predict_alert_time):\n",
    "  alert_data = data[data['cust_id'].isin(custinfo[custinfo['alert_key'].isin(predict_alert_time['alert_key'].tolist())]['cust_id'].tolist())]\n",
    "  train_data = data[~data['cust_id'].isin(custinfo[custinfo['alert_key'].isin(predict_alert_time['alert_key'].tolist())]['cust_id'].tolist())]\n",
    "  train_data['y'] = 0\n",
    "  sar_idx = train_data[train_data['cust_id'].isin(custinfo[custinfo['alert_key'].isin(y[y['sar_flag']==1]['alert_key'].tolist())]['cust_id'].tolist())].index\n",
    "  train_data.loc[sar_idx, 'y'] = 1\n",
    "  return train_data, alert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "q235FTxmizJR"
   },
   "outputs": [],
   "source": [
    "#Data Cleansing\n",
    "#SAR戶\b\b僅留前X天\n",
    "def train_prev_d(x, day):\n",
    "  prev_d = x.groupby('cust_id')['tx_date'].max() - day\n",
    "  prev_d = prev_d.reset_index()\n",
    "  prev_d.rename(columns={'tx_date':'prev_d'}, inplace=True)\n",
    "  x = x.merge(prev_d, on='cust_id', how='left')\n",
    "  x.drop(x[x['tx_date']<x['prev_d']].index,inplace=True)\n",
    "  x.pop('prev_d')\n",
    "  return x\n",
    "#特徵前處理\n",
    "def preprocess(data):\n",
    "  dict1 = {}\n",
    "  idx = 0\n",
    "  num = 0\n",
    "  for i in range(0,395,1):\n",
    "    dict1[i] = str(idx)\n",
    "    num += 1\n",
    "    if num == 7:\n",
    "      idx += 1\n",
    "      num = 0\n",
    "  data['tx_date_group'] = data.tx_date.map(lambda x: dict1[x])\n",
    "  data['session_cust_id'] = data.tx_date_group + data.cust_id\n",
    "  data['date_cust_id'] = data.tx_date.astype(str) + data.cust_id\n",
    "  data['date_time_cust_id'] = data.tx_date.astype(str) + data.tx_date.astype(str) + data.cust_id\n",
    "  data['tx_year'] = 2021\n",
    "  data['tx_date_formal'] = data['tx_date']+1\n",
    "  data.loc[data[data['tx_date_formal']>365].index,'tx_year'] = 2022\n",
    "  data.loc[data[data['tx_date_formal']>365].index, 'tx_date_formal'] = \\\n",
    "  data.loc[data[data['tx_date_formal']>365].index, 'tx_date_formal'] -365\n",
    "  data[\"tx_date_formal\"] = data[\"tx_year\"]*1000 + data[\"tx_date_formal\"]\n",
    "  data['tx_date_formal'] = pd.to_datetime(data['tx_date_formal'], format=\"%Y%j\")\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "DsaTx2WdcDcS"
   },
   "outputs": [],
   "source": [
    "def cdtx_feature_func(data):\n",
    "  ##(t-1)d diff\n",
    "  #data['prev_d_diff'] = data['trans_date'] - data.groupby('cust_id')['trans_date'].shift(1)\n",
    "  #data['prev_d_diff'].fillna(15, inplace=True)\n",
    "  #amt\n",
    "  all_amt_sum = (data.groupby('cust_id')['amt'].sum()/data.groupby('cust_id')['amt'].sum().max())\\\n",
    "              .rename('all_amt_sum').reset_index()\n",
    "  data = data.merge(all_amt_sum, on='cust_id', how='left')\n",
    "  date_amt_sum = (data.groupby(['cust_id','date'])['amt'].sum()/data.groupby(['cust_id','date'])['amt'].sum().max())\\\n",
    "              .rename('date_amt_sum').reset_index()\n",
    "  data = data.merge(date_amt_sum, on=['cust_id','date'], how='left')\n",
    "  #txn_cnt\n",
    "  all_txn_cnt = data.groupby('cust_id')['date'].count().rename('cdtx_all_txn_cnt').reset_index()\n",
    "  data = data.merge(all_txn_cnt, on='cust_id', how='left')\n",
    "  data['cdtx_all_txn_cnt'].fillna(0, inplace=True)\n",
    "  date_txn_cnt = data.groupby(['cust_id','date'])['amt'].count().rename('date_txn_cnt').reset_index()\n",
    "  data = data.merge(date_txn_cnt, on=['cust_id','date'], how='left')\n",
    "  #country\n",
    "  all_country_cnt = data.groupby('cust_id')['country'].count().rename('cdtx_all_country_cnt').reset_index()\n",
    "  data = data.merge(all_country_cnt, on='cust_id', how='left')\n",
    "  data['cdtx_all_country_cnt'].fillna(0, inplace=True)\n",
    "  date_country_cnt = data.groupby(['cust_id','date'])['country'].count().rename('date_country_cnt').reset_index()\n",
    "  data = data.merge(date_country_cnt, on=['cust_id','date'], how='left')\n",
    "  #cur_type\n",
    "  all_cur_type_cnt = data.groupby('cust_id')['cur_type'].count().rename('cdtx_all_cur_type_cnt').reset_index()\n",
    "  data = data.merge(all_cur_type_cnt, on='cust_id', how='left')\n",
    "  data['cdtx_all_cur_type_cnt'].fillna(0, inplace=True)\n",
    "  date_cur_type_cnt = data.groupby(['cust_id','date'])['cur_type'].count().rename('date_cur_type_cnt').reset_index()\n",
    "  data = data.merge(date_cur_type_cnt, on=['cust_id','date'], how='left')\n",
    "  #max_min_date_diff\n",
    "  max_min_date_diff = data.groupby(['cust_id']).apply(lambda x: x['date'].max() - x['date'].min()).rename('max_min_date_diff').reset_index()\n",
    "  data = data.merge(max_min_date_diff, on='cust_id', how='left')\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "g7Nxfaet2viO"
   },
   "outputs": [],
   "source": [
    "def remit_feature_func(data):\n",
    "  ##(t-1)d diff\n",
    "  #data['prev_d_diff'] = data['trans_date'] - data.groupby('cust_id')['trans_date'].shift(1)\n",
    "  #data['prev_d_diff'].fillna(15, inplace=True)\n",
    "  #amt\n",
    "  all_amt_sum = (data.groupby('cust_id')['trade_amount_usd'].sum()/data.groupby('cust_id')['trade_amount_usd'].sum().max())\\\n",
    "              .rename('all_amt_sum').reset_index()\n",
    "  data = data.merge(all_amt_sum, on='cust_id', how='left')\n",
    "  time_amt_sum = (data.groupby(['cust_id','trans_date'])['trade_amount_usd'].sum()/data.groupby(['cust_id','trans_date'])['trade_amount_usd'].sum().max())\\\n",
    "              .rename('time_amt_sum').reset_index()\n",
    "  data = data.merge(time_amt_sum, on=['cust_id','trans_date'], how='left')\n",
    "  data['trans_date'] = data['trans_date']/data['trans_date'].max()\n",
    "  #txn_cnt\n",
    "  all_txn_cnt = data.groupby('cust_id')['trans_date'].count().rename('remit_all_txn_cnt').reset_index()\n",
    "  data = data.merge(all_txn_cnt, on='cust_id', how='left')\n",
    "  data['remit_all_txn_cnt'].fillna(0, inplace=True)\n",
    "  date_txn_cnt = data.groupby(['cust_id','trans_date'])['trans_no'].count().rename('date_txn_cnt').reset_index()\n",
    "  data = data.merge(date_txn_cnt, on=['cust_id','trans_date'], how='left')\n",
    "  #max_min_date_diff\n",
    "  max_min_date_diff = data.groupby(['cust_id']).apply(lambda x: x['trans_date'].max() - x['trans_date'].min()).rename('max_min_date_diff').reset_index()\n",
    "  data = data.merge(max_min_date_diff, on='cust_id', how='left')\n",
    "  remit_pivot = data.groupby(['cust_id','trans_date','trans_no'])['trade_amount_usd'].sum().reset_index().pivot_table(index=['cust_id','trans_date'],columns='trans_no',values='trade_amount_usd').reset_index()\n",
    "  remit_pivot.fillna(0, inplace=True)\n",
    "  remit_col = list(remit_pivot.columns)\n",
    "  remit_col.remove('cust_id')\n",
    "  remit_col.remove('trans_date')\n",
    "  remit_pivot['remit_txn_sum'] = remit_pivot[remit_col].sum(axis=1)\n",
    "  for col in remit_col:\n",
    "    remit_pivot[col] = remit_pivot[col]/remit_pivot['remit_txn_sum']\n",
    "  for col in list(set([num for num in range(0,5,1)]) -  set(remit_pivot.columns[2:])):\n",
    "    remit_pivot[col] = 0.0\n",
    "  remit_pivot.rename(columns={0:'remit_trans_no_0',1:'remit_trans_no_1',2:'remit_trans_no_2',3:'remit_trans_no_3',4:'remit_trans_no_4'}, inplace=True)\n",
    "  remit_pivot.fillna(0, inplace=True)\n",
    "  data = data.merge(remit_pivot, on=['cust_id', 'trans_date'], how='left')\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "tnmkUyO5ng-R"
   },
   "outputs": [],
   "source": [
    "#session 交易差額比率\n",
    "def dp_feature_func(data):\n",
    "  session_amt_diff = data.groupby(['session_cust_id','debit_credit'])['tx_amt'].sum().reset_index()\n",
    "  session_amt_diff = pd.pivot_table(session_amt_diff, index='session_cust_id', columns='debit_credit', values='tx_amt')\n",
    "  session_amt_diff.fillna(1, inplace=True)\n",
    "  session_amt_diff['session_amt_diff_ratio'] = \\\n",
    "    abs(session_amt_diff['CR'] - session_amt_diff['DB']) / abs(session_amt_diff['CR'] + session_amt_diff['DB'])\n",
    "  session_amt_diff = session_amt_diff.reset_index()[['session_cust_id','session_amt_diff_ratio']]\n",
    "  data = data.merge(session_amt_diff, on='session_cust_id', how='left')\n",
    "  #當日 交易差額比率\n",
    "  date_amt_diff = data.groupby(['date_cust_id','debit_credit'])['tx_amt'].sum().reset_index()\n",
    "  date_amt_diff = pd.pivot_table(date_amt_diff, index='date_cust_id', columns='debit_credit', values='tx_amt')\n",
    "  date_amt_diff.fillna(1, inplace=True)\n",
    "  date_amt_diff['date_amt_diff_ratio'] = \\\n",
    "  abs(date_amt_diff['CR'] - date_amt_diff['DB']) / abs(date_amt_diff['CR'] + date_amt_diff['DB'])\n",
    "  date_amt_diff = date_amt_diff.reset_index()[['date_cust_id','date_amt_diff_ratio']]\n",
    "  data = data.merge(date_amt_diff, on=['date_cust_id'], how='left')\n",
    "  #當時 交易差額比率\n",
    "  date_time_amt_diff = data.groupby(['date_time_cust_id','debit_credit'])['tx_amt'].sum().reset_index()\n",
    "  date_time_amt_diff = pd.pivot_table(date_time_amt_diff, index='date_time_cust_id', columns='debit_credit', values='tx_amt')\n",
    "  date_time_amt_diff.fillna(1, inplace=True)\n",
    "  date_time_amt_diff['date_time_amt_diff_ratio'] = \\\n",
    "  abs(date_time_amt_diff['CR'] - date_time_amt_diff['DB']) / abs(date_time_amt_diff['CR'] + date_time_amt_diff['DB'])\n",
    "  date_time_amt_diff = date_time_amt_diff.reset_index()[['date_time_cust_id','date_time_amt_diff_ratio']]\n",
    "  data = data.merge(date_time_amt_diff, on=['date_time_cust_id'], how='left')\n",
    "  #當時交易筆數 tx_cnt_date_time\n",
    "  tx_cnt_date_time = data.groupby(['cust_id','tx_date','tx_time'])['debit_credit'].count().reset_index()\n",
    "  tx_cnt_date_time.rename(columns={'debit_credit':'tx_cnt_date_time'}, inplace=True)\n",
    "  data = data.merge(tx_cnt_date_time, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "  #當日交易筆數 tx_cnt_date\n",
    "  tx_cnt_date = data.groupby(['cust_id','tx_date'])['debit_credit'].count().reset_index()\n",
    "  tx_cnt_date.rename(columns={'debit_credit':'tx_cnt_date'}, inplace=True)\n",
    "  data = data.merge(tx_cnt_date, on=['cust_id','tx_date'], how='left')\n",
    "  #當時總分行數 txbranch_day_cnt\n",
    "  txbranch_day_time_cnt = data.groupby(['cust_id','tx_date','tx_time'])['txbranch'].count().reset_index()\n",
    "  txbranch_day_time_cnt.rename(columns={'txbranch':'txbranch_day_time_cnt'}, inplace=True)\n",
    "  data = data.merge(txbranch_day_time_cnt, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "  #單日總分行數 txbranch_day_cnt\n",
    "  txbranch_day_cnt = data.groupby(['cust_id','tx_date'])['txbranch'].count().reset_index()\n",
    "  txbranch_day_cnt.rename(columns={'txbranch':'txbranch_day_cnt'}, inplace=True)\n",
    "  data = data.merge(txbranch_day_cnt, on=['cust_id','tx_date'], how='left')\n",
    "  #當日ATM 佔交易數比例\n",
    "  day_atm_txn_ratio = data.groupby(['cust_id','tx_date'])['ATM'].sum().reset_index()\n",
    "  day_atm_txn_ratio.rename(columns={'ATM':'day_atm_txn_ratio'}, inplace=True)\n",
    "  data = data.merge(day_atm_txn_ratio, on=['cust_id','tx_date'], how='left')\n",
    "  data.day_atm_txn_ratio = data.day_atm_txn_ratio / data.tx_cnt_date\n",
    "  #當時ATM 佔交易數比例\n",
    "  day_time_atm_txn_ratio = data.groupby(['cust_id','tx_date','tx_time'])['ATM'].sum().reset_index()\n",
    "  day_time_atm_txn_ratio.rename(columns={'ATM':'day_time_atm_txn_ratio'}, inplace=True)\n",
    "  data = data.merge(day_time_atm_txn_ratio, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "  data.day_time_atm_txn_ratio = data.day_time_atm_txn_ratio / data.tx_cnt_date_time\n",
    "  #當日跨行 佔交易數比例\n",
    "  day_cross_bank_ratio = data.groupby(['cust_id','tx_date'])['cross_bank'].sum().reset_index()\n",
    "  day_cross_bank_ratio.rename(columns={'cross_bank':'day_cross_bank_ratio'}, inplace=True)\n",
    "  data = data.merge(day_cross_bank_ratio, on=['cust_id','tx_date'], how='left')\n",
    "  data.day_cross_bank_ratio = data.day_cross_bank_ratio / data.tx_cnt_date\n",
    "  #當時跨行 佔交易數比例\n",
    "  day_time_cross_bank_ratio = data.groupby(['cust_id','tx_date','tx_time'])['cross_bank'].sum().reset_index()\n",
    "  day_time_cross_bank_ratio.rename(columns={'cross_bank':'day_time_cross_bank_ratio'}, inplace=True)\n",
    "  data = data.merge(day_time_cross_bank_ratio, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "  data.day_time_cross_bank_ratio = data.day_time_cross_bank_ratio / data.tx_cnt_date_time\n",
    "\n",
    "  ##\n",
    "  #當session交易筆數 tx_cnt_session\n",
    "  tx_cnt_session = data.groupby(['session_cust_id'])['debit_credit'].count().reset_index()\n",
    "  tx_cnt_session.rename(columns={'debit_credit':'tx_cnt_session'}, inplace=True)\n",
    "  data = data.merge(tx_cnt_session, on=['session_cust_id'], how='left')\n",
    "  #當session總分行數 txbranch_session_cnt\n",
    "  txbranch_session_cnt = data.groupby(['session_cust_id'])['txbranch'].count().reset_index()\n",
    "  txbranch_session_cnt.rename(columns={'txbranch':'txbranch_session_cnt'}, inplace=True)\n",
    "  data = data.merge(txbranch_session_cnt, on=['session_cust_id'], how='left')\n",
    "  #當session跨行 佔交易數比例\n",
    "  session_cross_bank_ratio = data.groupby(['session_cust_id'])['cross_bank'].sum().reset_index()\n",
    "  session_cross_bank_ratio.rename(columns={'cross_bank':'session_cross_bank_ratio'}, inplace=True)\n",
    "  data = data.merge(session_cross_bank_ratio, on=['session_cust_id'], how='left')\n",
    "  data.session_cross_bank_ratio = data.session_cross_bank_ratio / data.tx_cnt_date\n",
    "  #當sessionATM 佔交易數比例\n",
    "  session_atm_txn_ratio = data.groupby(['session_cust_id'])['ATM'].sum().reset_index()\n",
    "  session_atm_txn_ratio.rename(columns={'ATM':'session_atm_txn_ratio'}, inplace=True)\n",
    "  data = data.merge(session_atm_txn_ratio, on=['session_cust_id'], how='left')\n",
    "  data.session_atm_txn_ratio = data.session_atm_txn_ratio / data.tx_cnt_date\n",
    "  #time_diff\n",
    "  distinct_date_time = data[['cust_id','tx_date','tx_time']].drop_duplicates().sort_values(['cust_id','tx_date','tx_time']).reset_index(drop=True)\n",
    "  distinct_date_time['date_diff'] = distinct_date_time.groupby('cust_id').apply(lambda x: x['tx_date'] - x['tx_date'].shift(1)).reset_index(drop=True)\n",
    "  distinct_date_time['time_diff'] = distinct_date_time.groupby('cust_id').apply(lambda x: x['tx_time'] - x['tx_time'].shift(1)).reset_index(drop=True)\n",
    "  distinct_date_time.fillna(0, inplace=True)\n",
    "  distinct_date_time['time_diff'] = (distinct_date_time['date_diff']*24) + distinct_date_time['time_diff']\n",
    "  data = data.merge(distinct_date_time[['cust_id', 'tx_date', 'tx_time', 'time_diff']], on=['cust_id', 'tx_date', 'tx_time'], how='left')\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "kbibYxDPdbzv"
   },
   "outputs": [],
   "source": [
    "def model_training_1(data, columns):\n",
    "  #逐筆交易處理\n",
    "  Y = data['y']\n",
    "  X = data[columns[1:-1]]\n",
    "  test_size = 0.2\n",
    "  seed = 42\n",
    "#   X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)#, stratify = Y)\n",
    "  model = XGBClassifier(\n",
    "                    base_score= 0.5, \n",
    "                booster= 'gbtree', \n",
    "                colsample_bylevel= 1, \n",
    "                colsample_bynode= 1, \n",
    "                colsample_bytree= 1, \n",
    "                gamma= 0, \n",
    "                learning_rate= 0.03,\n",
    "                max_delta_step= 0, \n",
    "                max_depth= 2, \n",
    "                min_child_weight= 1, \n",
    "#                 missing= None, \n",
    "                n_estimators= 90, \n",
    "                nthread= 16, \n",
    "                objective= 'binary:logistic', \n",
    "                reg_alpha= 0, \n",
    "                reg_lambda= 1, \n",
    "                scale_pos_weight= 1, \n",
    "                seed= 0, \n",
    "                subsample= 0.9,\n",
    "                verbosity= 1\n",
    "  )\n",
    "  model.fit(X, Y)\n",
    "#   y_pred = model.predict(X_test)\n",
    "#   predictions = [round(value) for value in y_pred]\n",
    "  # evaluate predictions\n",
    "#   accuracy = accuracy_score(y_test, predictions)\n",
    "#   print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "#   f1 = f1_score(y_test, predictions)\n",
    "#   print(\"F1 Scoure: %.2f%%\" % (f1 * 100.0))\n",
    "#   print(precision_score(y_test, predictions))\n",
    "#   print(recall_score(y_test, predictions))\n",
    "  feature_importance = pd.DataFrame({'columns':list(X.columns),'score':model.feature_importances_})\n",
    "  return X, model, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "anmklWd_dqpY"
   },
   "outputs": [],
   "source": [
    "#彙整model training 1偵測結果\n",
    "#predict_proba以0.05機率區間為一個欄位判斷分佈\n",
    "def mapping(proba):\n",
    "    if proba <= 0.1:\n",
    "        return 1\n",
    "    elif 0.1 < proba <= 0.2:\n",
    "        return 2\n",
    "    elif 0.2 < proba <= 0.3:\n",
    "        return 3\n",
    "    elif 0.3 < proba <= 0.4:\n",
    "        return 4\n",
    "    elif 0.4 < proba <= 0.5:\n",
    "        return 5\n",
    "    elif 0.5 < proba <= 0.6:\n",
    "        return 6\n",
    "    elif 0.6 < proba <= 0.7:\n",
    "        return 7\n",
    "    elif 0.7 < proba <= 0.8:\n",
    "        return 8\n",
    "    elif 0.8 < proba <= 0.9:\n",
    "        return 9\n",
    "    elif 0.9 < proba <= 1:\n",
    "        return 10\n",
    "def debit_credit_ratio_func(data):\n",
    "  #id\n",
    "  debit_credit_ratio = data.groupby(['cust_id'])['debit_credit'].value_counts().rename('debit_credit_ratio').reset_index()\n",
    "  debit_credit_ratio = debit_credit_ratio.pivot_table(values='debit_credit_ratio', index=['cust_id'], columns='debit_credit')\n",
    "  debit_credit_ratio.fillna(0, inplace=True)\n",
    "  debit_credit_ratio['debit_credit_ratio'] = debit_credit_ratio['DB']/debit_credit_ratio.sum(axis=1)\n",
    "  debit_credit_ratio = debit_credit_ratio.reset_index()[['cust_id','debit_credit_ratio']]\n",
    "  data = data.merge(debit_credit_ratio, on=['cust_id'], how='left')\n",
    "  return debit_credit_ratio\n",
    "def all_txn_cnt(data):\n",
    "  all_txn_cnt = data.groupby('cust_id')['tx_date'].count().rename('all_txn_cnt').reset_index()\n",
    "  data = data.merge(all_txn_cnt, on='cust_id', how='left')\n",
    "  return all_txn_cnt\n",
    "def result_preprocess_func(data, X, model):\n",
    "  data_db_cr_ratio = debit_credit_ratio_func(data)\n",
    "  data_all_txn_cnt = all_txn_cnt(data)\n",
    "  data['proba'] = model.predict_proba(X)[:,1]\n",
    "  result = data[['cust_id','proba','y']]\n",
    "  result['level'] = result[\"proba\"].map(mapping)\n",
    "  result = result[['cust_id', 'level', 'y']]\n",
    "  result = result.groupby(['cust_id','level']).count().reset_index().pivot_table(index='cust_id', columns='level', values='y')\n",
    "  result.fillna(0, inplace=True)\n",
    "  result = result.div(result.sum(axis=1), axis=0).reset_index()\n",
    "  for col in list(set([num for num in range(1,11,1)]) -  set(result.columns[1:])):\n",
    "    result[col] = 0.0\n",
    "  result = result[['cust_id', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
    "  result = result.merge(data[['cust_id','y']].drop_duplicates(), on='cust_id', how='left')\n",
    "  result = result.merge(data_db_cr_ratio, on='cust_id', how='left')\n",
    "  result = result.merge(data_all_txn_cnt, on='cust_id', how='left')\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "?XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = XGBClassifier(\n",
    "                    base_score= 0.5, \n",
    "                booster= 'gbtree', \n",
    "                colsample_bylevel= 1, \n",
    "                colsample_bynode= 1, \n",
    "                colsample_bytree= 1, \n",
    "                gamma= 0, \n",
    "                learning_rate= 0.2,\n",
    "                max_delta_step= 0, \n",
    "                max_depth= 2, \n",
    "                min_child_weight= 1, \n",
    "                n_estimators=80, \n",
    "                nthread= 16, \n",
    "                objective= 'binary:logistic', \n",
    "                reg_alpha= 0.1, \n",
    "                reg_lambda= 1, \n",
    "                scale_pos_weight= 1, \n",
    "                seed= 0, \n",
    "                subsample= 1,\n",
    "                verbosity= 1)\n",
    "model2 = lgb.LGBMClassifier(learning_rate = 0.1\n",
    "                             , max_depth=2\n",
    "                             , reg_lambda=1\n",
    "                             , n_estimators=70\n",
    "                             , reg_alpha=0.01)\n",
    "model3 = cb.CatBoostClassifier(\n",
    "                            learning_rate = 0.3\n",
    "                             , max_depth=2\n",
    "                             , reg_lambda=1\n",
    "                             , n_estimators=100\n",
    "                             , subsample = 1)\n",
    "model5 = GradientBoostingClassifier(\n",
    "                learning_rate= 0.2,\n",
    "                max_depth= 2, \n",
    "                n_estimators=100)\n",
    "\n",
    "model6 = HistGradientBoostingClassifier(                \n",
    "              learning_rate= 0.05,\n",
    "                max_depth= 2, \n",
    "                max_iter=210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "-Tv2I5j8du3O"
   },
   "outputs": [],
   "source": [
    "#model training 2\n",
    "#歸戶判斷是否報SAR\n",
    "def model_training_2(result):\n",
    "    result_col = list(result.columns)\n",
    "    result_col.remove('cust_id')\n",
    "    result_col.remove('y')\n",
    "    test_size = 0.2\n",
    "    seed = 42\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(result[result_col], result['y'], test_size=test_size, random_state=seed)#, stratify = result['y'])\n",
    "    estimators = [\n",
    "        ('xgb', model1),\n",
    "        ('lgbm', model2),\n",
    "#         ('cat', model3),\n",
    "#         ('ng', nb.NGBClassifier(\n",
    "#           n_estimators = 500\n",
    "#         , Base = DecisionTreeRegressor(criterion='friedman_mse', max_depth=5)\n",
    "#         , learning_rate = 0.05\n",
    "#                                           )),\n",
    "#         ('gd', model5),\n",
    "        ('hgbm', model6)\n",
    "    ]\n",
    "    model = StackingClassifier(\n",
    "        estimators=estimators, stack_method='predict_proba'\n",
    "        , final_estimator= HistGradientBoostingClassifier(                \n",
    "                            learning_rate= 0.05,\n",
    "                            max_depth= 2, \n",
    "                            max_iter=210)\n",
    "    )\n",
    "\n",
    "    model.fit(result[result_col], result['y'])\n",
    "#   y_pred = model.predict(X_test)\n",
    "#   predictions = [round(value) for value in y_pred]\n",
    "#   # evaluate predictions\n",
    "#   accuracy = accuracy_score(y_test, predictions)\n",
    "#   print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "#   f1 = f1_score(y_test, predictions)\n",
    "#   print(\"F1 Scoure: %.2f%%\" % (f1 * 100.0))\n",
    "#   print(precision_score(y_test, predictions))\n",
    "#   print(recall_score(y_test, predictions))\n",
    "    return model, result_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "E41cMxVIjkaR"
   },
   "outputs": [],
   "source": [
    "def alert_output(alert, model_1, model_2, alert_col, result_col, doc):\n",
    "  data_db_cr_ratio = debit_credit_ratio_func(alert)\n",
    "  data_all_txn_cnt = all_txn_cnt(alert)\n",
    "\n",
    "  #Remit Submission\n",
    "  alert_data = alert\n",
    "  alert_data['proba'] = model_1.predict_proba(alert[alert_col])[:,1]\n",
    "  alert_result = alert_data[['cust_id','proba']]\n",
    "  alert_result['level'] = alert_result[\"proba\"].map(mapping)\n",
    "  alert_result = alert_result[['cust_id', 'level']]\n",
    "  alert_result['cnt'] = 1\n",
    "  alert_result = alert_result.groupby(['cust_id','level'])['cnt'].count().reset_index().pivot_table(index='cust_id', columns='level', values='cnt')\n",
    "  alert_result.fillna(0, inplace=True)\n",
    "  alert_result = alert_result.div(alert_result.sum(axis=1), axis=0)\n",
    "  alert_result = alert_result.reset_index()\n",
    "  for col in list(set([num for num in range(1,11,1)]) -  set(alert_result.columns[1:])):\n",
    "    alert_result[col] = 0.0\n",
    "  alert_result = alert_result.merge(data_db_cr_ratio, on='cust_id', how='left')\n",
    "  alert_result = alert_result.merge(data_all_txn_cnt, on='cust_id', how='left')\n",
    "  alert_pred = model_2.predict_proba(alert_result[result_col])\n",
    "  # evaluate predictions\n",
    "  alert_result = alert_result[['cust_id']]\n",
    "  alert_result['probability'] = alert_pred[:,1]\n",
    "\n",
    "  final = predict_alert_time.merge(custinfo[['alert_key', 'cust_id']].merge(alert_result, on='cust_id'), on='alert_key', how='left')[['alert_key', 'probability']]\n",
    "  doc = doc[['alert_key']]\n",
    "  final = doc.merge(final, on='alert_key', how='left')\n",
    "  final.fillna(0,inplace=True)\n",
    "  final.loc[final[final['alert_key'].isin(prev_list)].index,'probability']=0\n",
    "  return final, alert_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "u0_a1IO05ZWZ"
   },
   "outputs": [],
   "source": [
    "def prev_7d_feature_func(data):\n",
    "  #last 7 days processing\n",
    "  data_distinct = data[['cust_id', 'tx_date_formal','tx_date']].drop_duplicates().reset_index(drop=True)\n",
    "  cross_bank_sum = data.groupby(['cust_id','tx_date_formal'])['cross_bank'].sum().rename('cross_bank_sum').reset_index()\n",
    "  prev_7d_data = data_distinct.merge(cross_bank_sum, on=['cust_id','tx_date_formal'], how='left')\n",
    "  dbcr_amt_sum = data.groupby(['cust_id','tx_date_formal','debit_credit'])['tx_amt'].sum().rename('debit_credit_sum').reset_index()\n",
    "  dbcr_amt_sum = pd.pivot_table(dbcr_amt_sum, index=['cust_id','tx_date_formal'], columns='debit_credit', values='debit_credit_sum')\n",
    "  prev_7d_data = prev_7d_data.merge(dbcr_amt_sum, on=['cust_id','tx_date_formal'], how='left')\n",
    "  tx_date_sum = data.groupby(['cust_id','tx_date_formal'])['tx_amt'].sum().rename('tx_amt_sum')\n",
    "  prev_7d_data = prev_7d_data.merge(tx_date_sum, on=['cust_id','tx_date_formal'], how='left')\n",
    "  tx_cnt = data.groupby(['cust_id','tx_date_formal'])['tx_amt'].count().rename('tx_cnt').reset_index()\n",
    "  prev_7d_data = prev_7d_data.merge(tx_cnt, on=['cust_id','tx_date_formal'], how='left')\n",
    "  #當日ATM 佔交易數比例\n",
    "  day_atm_cnt = data.groupby(['cust_id','tx_date_formal'])['ATM'].sum().rename('day_atm_cnt').reset_index()\n",
    "  prev_7d_data = prev_7d_data.merge(day_atm_cnt, on=['cust_id','tx_date_formal'], how='left')\n",
    "  txbranch_day_cnt = data.groupby(['cust_id','tx_date_formal'])['txbranch'].count().rename('txbranch_day_cnt').reset_index()\n",
    "  prev_7d_data = prev_7d_data.merge(txbranch_day_cnt, on=['cust_id','tx_date_formal'], how='left')\n",
    "  prev_7d_data.fillna(0, inplace=True)\n",
    "  prev_7d_data = prev_7d_data.sort_values(['cust_id','tx_date_formal'])\n",
    "  prev_7d_data = prev_7d_data.set_index('tx_date_formal')\n",
    "  #feature engineering\n",
    "  prev_7d_cross_bank_sum = prev_7d_data.groupby('cust_id')['cross_bank_sum'].rolling(window='7D').sum().rename('prev_7d_cross_bank_sum').reset_index()\n",
    "  prev_7d_CR_sum = prev_7d_data.groupby('cust_id')['CR'].rolling(window='7D').sum().rename('prev_7d_CR_sum').reset_index()\n",
    "  prev_7d_DB_sum = prev_7d_data.groupby('cust_id')['DB'].rolling(window='7D').sum().rename('prev_7d_DB_sum').reset_index()\n",
    "  prev_7d_tx_cnt = prev_7d_data.groupby('cust_id')['tx_cnt'].rolling(window='7D').sum().rename('prev_7d_tx_cnt').reset_index()\n",
    "  prev_7d_txbranch_cnt = prev_7d_data.groupby('cust_id')['txbranch_day_cnt'].rolling(window='7D').sum().rename('prev_7d_txbranch_cnt').reset_index()\n",
    "  prev_7d_atm_cnt = prev_7d_data.groupby('cust_id')['day_atm_cnt'].rolling(window='7D').sum().rename('prev_7d_atm_cnt').reset_index()\n",
    "\n",
    "  data_distinct = data_distinct.merge(prev_7d_CR_sum, on=['cust_id','tx_date_formal']).merge(prev_7d_DB_sum, on=['cust_id','tx_date_formal'])\\\n",
    "    .merge(prev_7d_tx_cnt, on=['cust_id','tx_date_formal']).merge(prev_7d_txbranch_cnt, on=['cust_id','tx_date_formal']).merge(prev_7d_atm_cnt, on=['cust_id','tx_date_formal'])\\\n",
    "    .merge(prev_7d_cross_bank_sum, on=['cust_id','tx_date_formal'])\n",
    "\n",
    "  data_distinct['prev_7d_txbranch_ratio'] = data_distinct.prev_7d_txbranch_cnt / data_distinct.prev_7d_tx_cnt\n",
    "  data_distinct['prev_7d_atm_ratio'] = data_distinct.prev_7d_atm_cnt / data_distinct.prev_7d_tx_cnt\n",
    "  data_distinct['prev_7d_cross_bank_ratio'] = data_distinct.prev_7d_cross_bank_sum / data_distinct.prev_7d_tx_cnt\n",
    "  data_distinct['prev7d_amt_diff_ratio'] = \\\n",
    "    abs(data_distinct['prev_7d_CR_sum'] - data_distinct['prev_7d_DB_sum']) / abs(data_distinct['prev_7d_CR_sum'] + data_distinct['prev_7d_DB_sum'])\n",
    "  data_distinct = data_distinct[['cust_id','tx_date','prev_7d_tx_cnt','prev7d_amt_diff_ratio','prev_7d_txbranch_ratio','prev_7d_atm_cnt','prev_7d_atm_ratio',\n",
    "                                 'prev_7d_txbranch_cnt','prev_7d_cross_bank_ratio','prev_7d_cross_bank_sum']]\n",
    "  data = data.merge(data_distinct, on=['cust_id', 'tx_date'], how='left')\n",
    "  return data, data_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "FUdbYkG8o43l",
    "outputId": "f62944ec-42d6-491f-974d-87d45c1d3d94",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alert_key</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>357307</td>\n",
       "      <td>0.011147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>376329</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>373644</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>357668</td>\n",
       "      <td>0.043079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>354443</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>364485</td>\n",
       "      <td>0.037486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>363155</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>368710</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>358067</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>372119</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3850 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      alert_key  probability\n",
       "0        357307     0.011147\n",
       "1        376329     0.000000\n",
       "2        373644     0.000000\n",
       "3        357668     0.043079\n",
       "4        354443     0.000000\n",
       "...         ...          ...\n",
       "3845     364485     0.037486\n",
       "3846     363155     0.000000\n",
       "3847     368710     0.000000\n",
       "3848     358067     0.000000\n",
       "3849     372119     0.000000\n",
       "\n",
       "[3850 rows x 2 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dp zone\n",
    "train_dp, alert_dp = train_alert_process_func(dp, custinfo, predict_alert_time)\n",
    "train_dp = train_prev_d(train_dp, 30)\n",
    "alert_dp = train_prev_d(alert_dp, 30)\n",
    "train_dp = preprocess(train_dp)\n",
    "alert_dp = preprocess(alert_dp)\n",
    "##\n",
    "\n",
    "train_dp, train_dp_distinct = prev_7d_feature_func(train_dp)\n",
    "alert_dp, alert_dp_distinct = prev_7d_feature_func(alert_dp)\n",
    "##\n",
    "train_dp = dp_feature_func(train_dp)\n",
    "alert_dp = dp_feature_func(alert_dp)\n",
    "\n",
    "dp_col = ['cust_id','session_amt_diff_ratio', \n",
    "          #,'txbranch_session_cnt','session_atm_txn_ratio','session_cross_bank_ratio',\n",
    "          'date_time_amt_diff_ratio','tx_cnt_date_time','txbranch_day_time_cnt', 'day_time_atm_txn_ratio','day_time_cross_bank_ratio',\n",
    "          'date_amt_diff_ratio','tx_cnt_date','txbranch_day_cnt','day_atm_txn_ratio','day_cross_bank_ratio',#(version : original)\n",
    "          'time_diff',#(version : date_diff_30)\n",
    "#            'tx_cnt_session',#(version : tx_cnt_session)\n",
    "          #'prev7d_amt_diff_ratio',\n",
    "          #'prev_7d_tx_cnt','prev_7d_txbranch_ratio',#'prev_7d_atm_cnt','prev_7d_atm_ratio',\n",
    "          #'prev_7d_txbranch_cnt',#'prev_7d_cross_bank_sum',#'prev_7d_cross_bank_ratio',\n",
    "          'y']\n",
    "dp_X, dp_model_1, dp_feature_importance = model_training_1(train_dp, dp_col)\n",
    "dp_result = result_preprocess_func(train_dp, dp_X, dp_model_1)\n",
    "dp_model_2, dp_result_col = model_training_2(dp_result)\n",
    "dp_col.remove('y')\n",
    "dp_col.remove('cust_id')\n",
    "dp_final, alert_dp_result = alert_output(alert_dp, dp_model_1, dp_model_2, dp_col, dp_result_col, doc)\n",
    "dp_final\n",
    "#F1 Scoure: 66.66%\n",
    "#0.9980544747081712\n",
    "#0.5003657644476956\n",
    "#F1 Scoure: 28.57%\n",
    "#0.875\n",
    "#0.17073170731707318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "tW5_zPa6bg6h",
    "outputId": "bcf83957-7b15-4e46-9555-02adea4c094d",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>session_amt_diff_ratio</td>\n",
       "      <td>0.077227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date_time_amt_diff_ratio</td>\n",
       "      <td>0.111927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tx_cnt_date_time</td>\n",
       "      <td>0.129007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>txbranch_day_time_cnt</td>\n",
       "      <td>0.006482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day_time_atm_txn_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day_time_cross_bank_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>date_amt_diff_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tx_cnt_date</td>\n",
       "      <td>0.532249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>txbranch_day_cnt</td>\n",
       "      <td>0.092074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>day_atm_txn_ratio</td>\n",
       "      <td>0.051034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>day_cross_bank_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>time_diff</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      columns     score\n",
       "0      session_amt_diff_ratio  0.077227\n",
       "1    date_time_amt_diff_ratio  0.111927\n",
       "2            tx_cnt_date_time  0.129007\n",
       "3       txbranch_day_time_cnt  0.006482\n",
       "4      day_time_atm_txn_ratio  0.000000\n",
       "5   day_time_cross_bank_ratio  0.000000\n",
       "6         date_amt_diff_ratio  0.000000\n",
       "7                 tx_cnt_date  0.532249\n",
       "8            txbranch_day_cnt  0.092074\n",
       "9           day_atm_txn_ratio  0.051034\n",
       "10       day_cross_bank_ratio  0.000000\n",
       "11                  time_diff  0.000000"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "cGvCK_odkTnw"
   },
   "outputs": [],
   "source": [
    "dp_final.to_csv('stacking_HGBM_stacking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
