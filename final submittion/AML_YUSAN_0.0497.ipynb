{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqgjAeDc3fQO",
    "outputId": "a8582f79-532c-4ba2-8784-18cfc179fede"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da_features = pd.read_csv('da_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MmPXIGm33kZV"
   },
   "outputs": [],
   "source": [
    "#ccba\n",
    "ccba = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_ccba_full_hashed.csv')\n",
    "#cdtx\n",
    "cdtx = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_cdtx0001_full_hashed.csv')\n",
    "#custinfo\n",
    "custinfo = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_custinfo_full_hashed.csv')\n",
    "#dp\n",
    "dp = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_dp_full_hashed.csv')\n",
    "#remit\n",
    "remit = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_train_x_remit1_full_hashed.csv')\n",
    "#train_alert_time\n",
    "train_alert_time = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\train_x_alert_date.csv')\n",
    "#predict_alert_time\n",
    "predict_alert_time = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\public_x_alert_date.csv')\n",
    "#y\n",
    "y = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\train_y_answer.csv')\n",
    "#案件名單\n",
    "doc = pd.read_csv(r'E:\\Desktop\\玉山\\訓練資料集_first\\預測的案件名單及提交檔案範例.csv')\n",
    "#prev_list set to 0 before submission\n",
    "doc_merged = custinfo.merge(doc, on='alert_key', how='right').merge(predict_alert_time, on='alert_key', how='left')\n",
    "doc_merged = doc_merged[~doc_merged['cust_id'].isnull()]\n",
    "max_doc = doc_merged.groupby('cust_id')['date'].max().reset_index()\n",
    "max_doc.rename(columns={'date':'max_date'},inplace=True)\n",
    "doc_merged = doc_merged.merge(max_doc, on='cust_id', how='left')\n",
    "prev_list = doc_merged[doc_merged['max_date']>doc_merged['date']]['alert_key'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3o4rrFewaSw9"
   },
   "outputs": [],
   "source": [
    "def train_alert_process_func(data, custinfo, predict_alert_time):\n",
    "  alert_data = data[data['cust_id'].isin(custinfo[custinfo['alert_key'].isin(predict_alert_time['alert_key'].tolist())]['cust_id'].tolist())]\n",
    "  train_data = data[~data['cust_id'].isin(custinfo[custinfo['alert_key'].isin(predict_alert_time['alert_key'].tolist())]['cust_id'].tolist())]\n",
    "  train_data['y'] = 0\n",
    "  sar_idx = train_data[train_data['cust_id'].isin(custinfo[custinfo['alert_key'].isin(y[y['sar_flag']==1]['alert_key'].tolist())]['cust_id'].tolist())].index\n",
    "  train_data.loc[sar_idx, 'y'] = 1\n",
    "  return train_data, alert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "q235FTxmizJR"
   },
   "outputs": [],
   "source": [
    "#Data Cleansing\n",
    "#SAR戶\b\b僅留前X天\n",
    "def train_prev_d(x, day):\n",
    "  prev_d = x.groupby('cust_id')['tx_date'].max() - day\n",
    "  prev_d = prev_d.reset_index()\n",
    "  prev_d.rename(columns={'tx_date':'prev_d'}, inplace=True)\n",
    "  x = x.merge(prev_d, on='cust_id', how='left')\n",
    "  x.drop(x[x['tx_date']<x['prev_d']].index,inplace=True)\n",
    "  x.pop('prev_d')\n",
    "  return x\n",
    "#特徵前處理\n",
    "def preprocess(data):\n",
    "  dict1 = {}\n",
    "  idx = 0\n",
    "  num = 0\n",
    "  for i in range(0,395,1):\n",
    "    dict1[i] = str(idx)\n",
    "    num += 1\n",
    "    if num == 7:\n",
    "      idx += 1\n",
    "      num = 0\n",
    "  data['tx_date_group'] = data.tx_date.map(lambda x: dict1[x])\n",
    "  data['session_cust_id'] = data.tx_date_group + data.cust_id\n",
    "  data['date_cust_id'] = data.tx_date.astype(str) + data.cust_id\n",
    "  data['date_time_cust_id'] = data.tx_date.astype(str) + data.tx_date.astype(str) + data.cust_id\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "DsaTx2WdcDcS"
   },
   "outputs": [],
   "source": [
    "def cdtx_feature_func(data):\n",
    "  ##(t-1)d diff\n",
    "  #data['prev_d_diff'] = data['trans_date'] - data.groupby('cust_id')['trans_date'].shift(1)\n",
    "  #data['prev_d_diff'].fillna(15, inplace=True)\n",
    "  #amt\n",
    "  all_amt_sum = (data.groupby('cust_id')['amt'].sum()/data.groupby('cust_id')['amt'].sum().max())\\\n",
    "              .rename('all_amt_sum').reset_index()\n",
    "  data = data.merge(all_amt_sum, on='cust_id', how='left')\n",
    "  date_amt_sum = (data.groupby(['cust_id','date'])['amt'].sum()/data.groupby(['cust_id','date'])['amt'].sum().max())\\\n",
    "              .rename('date_amt_sum').reset_index()\n",
    "  data = data.merge(date_amt_sum, on=['cust_id','date'], how='left')\n",
    "  #txn_cnt\n",
    "  all_txn_cnt = data.groupby('cust_id')['date'].count().rename('cdtx_all_txn_cnt').reset_index()\n",
    "  data = data.merge(all_txn_cnt, on='cust_id', how='left')\n",
    "  data['cdtx_all_txn_cnt'].fillna(0, inplace=True)\n",
    "  date_txn_cnt = data.groupby(['cust_id','date'])['amt'].count().rename('date_txn_cnt').reset_index()\n",
    "  data = data.merge(date_txn_cnt, on=['cust_id','date'], how='left')\n",
    "  #country\n",
    "  all_country_cnt = data.groupby('cust_id')['country'].count().rename('cdtx_all_country_cnt').reset_index()\n",
    "  data = data.merge(all_country_cnt, on='cust_id', how='left')\n",
    "  data['cdtx_all_country_cnt'].fillna(0, inplace=True)\n",
    "  date_country_cnt = data.groupby(['cust_id','date'])['country'].count().rename('date_country_cnt').reset_index()\n",
    "  data = data.merge(date_country_cnt, on=['cust_id','date'], how='left')\n",
    "  #cur_type\n",
    "  all_cur_type_cnt = data.groupby('cust_id')['cur_type'].count().rename('cdtx_all_cur_type_cnt').reset_index()\n",
    "  data = data.merge(all_cur_type_cnt, on='cust_id', how='left')\n",
    "  data['cdtx_all_cur_type_cnt'].fillna(0, inplace=True)\n",
    "  date_cur_type_cnt = data.groupby(['cust_id','date'])['cur_type'].count().rename('date_cur_type_cnt').reset_index()\n",
    "  data = data.merge(date_cur_type_cnt, on=['cust_id','date'], how='left')\n",
    "  #max_min_date_diff\n",
    "  max_min_date_diff = data.groupby(['cust_id']).apply(lambda x: x['date'].max() - x['date'].min()).rename('max_min_date_diff').reset_index()\n",
    "  data = data.merge(max_min_date_diff, on='cust_id', how='left')\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "g7Nxfaet2viO"
   },
   "outputs": [],
   "source": [
    "def remit_feature_func(data):\n",
    "  ##(t-1)d diff\n",
    "  #data['prev_d_diff'] = data['trans_date'] - data.groupby('cust_id')['trans_date'].shift(1)\n",
    "  #data['prev_d_diff'].fillna(15, inplace=True)\n",
    "  #amt\n",
    "  all_amt_sum = (data.groupby('cust_id')['trade_amount_usd'].sum()/data.groupby('cust_id')['trade_amount_usd'].sum().max())\\\n",
    "              .rename('all_amt_sum').reset_index()\n",
    "  data = data.merge(all_amt_sum, on='cust_id', how='left')\n",
    "  time_amt_sum = (data.groupby(['cust_id','trans_date'])['trade_amount_usd'].sum()/data.groupby(['cust_id','trans_date'])['trade_amount_usd'].sum().max())\\\n",
    "              .rename('time_amt_sum').reset_index()\n",
    "  data = data.merge(time_amt_sum, on=['cust_id','trans_date'], how='left')\n",
    "  data['trans_date'] = data['trans_date']/data['trans_date'].max()\n",
    "  #txn_cnt\n",
    "  all_txn_cnt = data.groupby('cust_id')['trans_date'].count().rename('remit_all_txn_cnt').reset_index()\n",
    "  data = data.merge(all_txn_cnt, on='cust_id', how='left')\n",
    "  data['remit_all_txn_cnt'].fillna(0, inplace=True)\n",
    "  date_txn_cnt = data.groupby(['cust_id','trans_date'])['trans_no'].count().rename('date_txn_cnt').reset_index()\n",
    "  data = data.merge(date_txn_cnt, on=['cust_id','trans_date'], how='left')\n",
    "  #max_min_date_diff\n",
    "  max_min_date_diff = data.groupby(['cust_id']).apply(lambda x: x['trans_date'].max() - x['trans_date'].min()).rename('max_min_date_diff').reset_index()\n",
    "  data = data.merge(max_min_date_diff, on='cust_id', how='left')\n",
    "  remit_pivot = data.groupby(['cust_id','trans_date','trans_no'])['trade_amount_usd'].sum().reset_index().pivot_table(index=['cust_id','trans_date'],columns='trans_no',values='trade_amount_usd').reset_index()\n",
    "  remit_pivot.fillna(0, inplace=True)\n",
    "  remit_col = list(remit_pivot.columns)\n",
    "  remit_col.remove('cust_id')\n",
    "  remit_col.remove('trans_date')\n",
    "  remit_pivot['remit_txn_sum'] = remit_pivot[remit_col].sum(axis=1)\n",
    "  for col in remit_col:\n",
    "    remit_pivot[col] = remit_pivot[col]/remit_pivot['remit_txn_sum']\n",
    "  for col in list(set([num for num in range(0,5,1)]) -  set(remit_pivot.columns[2:])):\n",
    "    remit_pivot[col] = 0.0\n",
    "  remit_pivot.rename(columns={0:'remit_trans_no_0',1:'remit_trans_no_1',2:'remit_trans_no_2',3:'remit_trans_no_3',4:'remit_trans_no_4'}, inplace=True)\n",
    "  remit_pivot.fillna(0, inplace=True)\n",
    "  data = data.merge(remit_pivot, on=['cust_id', 'trans_date'], how='left')\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tnmkUyO5ng-R"
   },
   "outputs": [],
   "source": [
    "#session 交易差額比率\n",
    "def dp_feature_func(data):\n",
    "  session_amt_diff = data.groupby(['session_cust_id','debit_credit'])['tx_amt'].sum().reset_index()\n",
    "  session_amt_diff = pd.pivot_table(session_amt_diff, index='session_cust_id', columns='debit_credit', values='tx_amt')\n",
    "  session_amt_diff.fillna(1, inplace=True)\n",
    "  session_amt_diff['session_amt_diff_ratio'] = \\\n",
    "    abs(session_amt_diff['CR'] - session_amt_diff['DB']) / abs(session_amt_diff['CR'] + session_amt_diff['DB'])\n",
    "  session_amt_diff = session_amt_diff.reset_index()[['session_cust_id','session_amt_diff_ratio']]\n",
    "  data = data.merge(session_amt_diff, on='session_cust_id', how='left')\n",
    "#當日 交易差額比率\n",
    "  date_amt_diff = data.groupby(['date_cust_id','debit_credit'])['tx_amt'].sum().reset_index()\n",
    "  date_amt_diff = pd.pivot_table(date_amt_diff, index='date_cust_id', columns='debit_credit', values='tx_amt')\n",
    "  date_amt_diff.fillna(1, inplace=True)\n",
    "  date_amt_diff['date_amt_diff_ratio'] = \\\n",
    "  abs(date_amt_diff['CR'] - date_amt_diff['DB']) / abs(date_amt_diff['CR'] + date_amt_diff['DB'])\n",
    "  date_amt_diff = date_amt_diff.reset_index()[['date_cust_id','date_amt_diff_ratio']]\n",
    "  data = data.merge(date_amt_diff, on=['date_cust_id'], how='left')\n",
    "#當時 交易差額比率\n",
    "  date_time_amt_diff = data.groupby(['date_time_cust_id','debit_credit'])['tx_amt'].sum().reset_index()\n",
    "  date_time_amt_diff = pd.pivot_table(date_time_amt_diff, index='date_time_cust_id', columns='debit_credit', values='tx_amt')\n",
    "  date_time_amt_diff.fillna(1, inplace=True)\n",
    "  date_time_amt_diff['date_time_amt_diff_ratio'] = \\\n",
    "  abs(date_time_amt_diff['CR'] - date_time_amt_diff['DB']) / abs(date_time_amt_diff['CR'] + date_time_amt_diff['DB'])\n",
    "  date_time_amt_diff = date_time_amt_diff.reset_index()[['date_time_cust_id','date_time_amt_diff_ratio']]\n",
    "  data = data.merge(date_time_amt_diff, on=['date_time_cust_id'], how='left')\n",
    "#當時交易筆數 tx_cnt_date_time\n",
    "  tx_cnt_date_time = data.groupby(['cust_id','tx_date','tx_time'])['debit_credit'].count().reset_index()\n",
    "  tx_cnt_date_time.rename(columns={'debit_credit':'tx_cnt_date_time'}, inplace=True)\n",
    "  data = data.merge(tx_cnt_date_time, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "#當日交易筆數 tx_cnt_date\n",
    "  tx_cnt_date = data.groupby(['cust_id','tx_date'])['debit_credit'].count().reset_index()\n",
    "  tx_cnt_date.rename(columns={'debit_credit':'tx_cnt_date'}, inplace=True)\n",
    "  data = data.merge(tx_cnt_date, on=['cust_id','tx_date'], how='left')\n",
    "#當時總分行數 txbranch_day_cnt\n",
    "  txbranch_day_time_cnt = data.groupby(['cust_id','tx_date','tx_time'])['txbranch'].count().reset_index()\n",
    "  txbranch_day_time_cnt.rename(columns={'txbranch':'txbranch_day_time_cnt'}, inplace=True)\n",
    "  data = data.merge(txbranch_day_time_cnt, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "#單日總分行數 txbranch_day_cnt\n",
    "  txbranch_day_cnt = data.groupby(['cust_id','tx_date'])['txbranch'].count().reset_index()\n",
    "  txbranch_day_cnt.rename(columns={'txbranch':'txbranch_day_cnt'}, inplace=True)\n",
    "  data = data.merge(txbranch_day_cnt, on=['cust_id','tx_date'], how='left')\n",
    "#當日ATM 佔交易數比例\n",
    "  day_atm_txn_ratio = data.groupby(['cust_id','tx_date'])['ATM'].sum().reset_index()\n",
    "  day_atm_txn_ratio.rename(columns={'ATM':'day_atm_txn_ratio'}, inplace=True)\n",
    "  data = data.merge(day_atm_txn_ratio, on=['cust_id','tx_date'], how='left')\n",
    "  data.day_atm_txn_ratio = data.day_atm_txn_ratio / data.tx_cnt_date\n",
    "#當時ATM 佔交易數比例\n",
    "  day_time_atm_txn_ratio = data.groupby(['cust_id','tx_date','tx_time'])['ATM'].sum().reset_index()\n",
    "  day_time_atm_txn_ratio.rename(columns={'ATM':'day_time_atm_txn_ratio'}, inplace=True)\n",
    "  data = data.merge(day_time_atm_txn_ratio, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "  data.day_time_atm_txn_ratio = data.day_time_atm_txn_ratio / data.tx_cnt_date_time\n",
    "    #當日跨行 佔交易數比例\n",
    "  day_cross_bank_ratio = data.groupby(['cust_id','tx_date'])['cross_bank'].sum().reset_index()\n",
    "  day_cross_bank_ratio.rename(columns={'cross_bank':'day_cross_bank_ratio'}, inplace=True)\n",
    "  data = data.merge(day_cross_bank_ratio, on=['cust_id','tx_date'], how='left')\n",
    "  data.day_cross_bank_ratio = data.day_cross_bank_ratio / data.tx_cnt_date\n",
    "  #當時跨行 佔交易數比例\n",
    "  day_time_cross_bank_ratio = data.groupby(['cust_id','tx_date','tx_time'])['cross_bank'].sum().reset_index()\n",
    "  day_time_cross_bank_ratio.rename(columns={'cross_bank':'day_time_cross_bank_ratio'}, inplace=True)\n",
    "  data = data.merge(day_time_cross_bank_ratio, on=['cust_id','tx_date','tx_time'], how='left')\n",
    "  data.day_time_cross_bank_ratio = data.day_time_cross_bank_ratio / data.tx_cnt_date_time\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "kbibYxDPdbzv"
   },
   "outputs": [],
   "source": [
    "def model_training_1(data, columns):\n",
    "  #逐筆交易處理\n",
    "  Y = data['y']\n",
    "  X = data[columns[1:-1]]\n",
    "  test_size = 0.2\n",
    "  seed = 42\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)#, stratify = Y)\n",
    "  model = XGBClassifier(\n",
    "  \n",
    "        base_score= 0.5, \n",
    "        booster= 'gbtree', \n",
    "        colsample_bylevel= 1, \n",
    "        colsample_bynode= 1, \n",
    "        colsample_bytree= 1, \n",
    "        gamma= 0, \n",
    "        learning_rate= 0.1,\n",
    "        max_delta_step= 0, \n",
    "        max_depth= 3, \n",
    "        min_child_weight= 1, \n",
    "#                         missing= None, \n",
    "        n_estimators= 100, \n",
    "        nthread= 1, \n",
    "        objective= 'binary:logistic', \n",
    "        reg_alpha= 0, \n",
    "        reg_lambda= 1, \n",
    "        scale_pos_weight= 10, \n",
    "        seed= 0, \n",
    "        subsample= 1,\n",
    "        verbosity= 1  \n",
    "\n",
    "  )\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "  predictions = [round(value) for value in y_pred]\n",
    "  # evaluate predictions\n",
    "  accuracy = accuracy_score(y_test, predictions)\n",
    "  print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "  f1 = f1_score(y_test, predictions)\n",
    "  print(\"F1 Scoure: %.2f%%\" % (f1 * 100.0))\n",
    "  print(precision_score(y_test, predictions))\n",
    "  print(recall_score(y_test, predictions))\n",
    "  feature_importance = pd.DataFrame({'columns':list(X.columns),'score':model.feature_importances_})\n",
    "  return X, model, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "anmklWd_dqpY"
   },
   "outputs": [],
   "source": [
    "#彙整model training 1偵測結果\n",
    "#predict_proba以0.05機率區間為一個欄位判斷分佈\n",
    "def mapping(proba):\n",
    "    if proba <= 0.1:\n",
    "        return 1\n",
    "    elif 0.1 < proba <= 0.2:\n",
    "        return 2\n",
    "    elif 0.2 < proba <= 0.3:\n",
    "        return 3\n",
    "    elif 0.3 < proba <= 0.4:\n",
    "        return 4\n",
    "    elif 0.4 < proba <= 0.5:\n",
    "        return 5\n",
    "    elif 0.5 < proba <= 0.6:\n",
    "        return 6\n",
    "    elif 0.6 < proba <= 0.7:\n",
    "        return 7\n",
    "    elif 0.7 < proba <= 0.8:\n",
    "        return 8\n",
    "    elif 0.8 < proba <= 0.9:\n",
    "        return 9\n",
    "    elif 0.9 < proba <= 1:\n",
    "        return 10\n",
    "def debit_credit_ratio_func(data):\n",
    "  #id\n",
    "  debit_credit_ratio = data.groupby(['cust_id'])['debit_credit'].value_counts().rename('debit_credit_ratio').reset_index()\n",
    "  debit_credit_ratio = debit_credit_ratio.pivot_table(values='debit_credit_ratio', index=['cust_id'], columns='debit_credit')\n",
    "  debit_credit_ratio.fillna(0, inplace=True)\n",
    "  debit_credit_ratio['debit_credit_ratio'] = debit_credit_ratio['DB']/debit_credit_ratio.sum(axis=1)\n",
    "  debit_credit_ratio = debit_credit_ratio.reset_index()[['cust_id','debit_credit_ratio']]\n",
    "  data = data.merge(debit_credit_ratio, on=['cust_id'], how='left')\n",
    "  return debit_credit_ratio\n",
    "def all_txn_cnt(data):\n",
    "  all_txn_cnt = data.groupby('cust_id')['tx_date'].count().rename('all_txn_cnt').reset_index()\n",
    "  data = data.merge(all_txn_cnt, on='cust_id', how='left')\n",
    "  return all_txn_cnt\n",
    "def result_preprocess_func(data, X, model):\n",
    "  data_db_cr_ratio = debit_credit_ratio_func(data)\n",
    "  data_all_txn_cnt = all_txn_cnt(data)\n",
    "  data['proba'] = model.predict_proba(X)[:,1]\n",
    "  result = data[['cust_id','proba','y']]\n",
    "  result['level'] = result[\"proba\"].map(mapping)\n",
    "  result = result[['cust_id', 'level', 'y']]\n",
    "  result = result.groupby(['cust_id','level']).count().reset_index().pivot_table(index='cust_id', columns='level', values='y')\n",
    "  result.fillna(0, inplace=True)\n",
    "  result = result.div(result.sum(axis=1), axis=0).reset_index()\n",
    "  for col in list(set([num for num in range(1,11,1)]) -  set(result.columns[1:])):\n",
    "    result[col] = 0.0\n",
    "  result = result[['cust_id', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
    "  result = result.merge(data[['cust_id','y']].drop_duplicates(), on='cust_id', how='left')\n",
    "  result = result.merge(data_db_cr_ratio, on='cust_id', how='left')\n",
    "  result = result.merge(data_all_txn_cnt, on='cust_id', how='left')\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-Tv2I5j8du3O"
   },
   "outputs": [],
   "source": [
    "#model training 2\n",
    "#歸戶判斷是否報SAR\n",
    "def model_training_2(result):\n",
    "  result_col = list(result.columns)\n",
    "  result_col.remove('cust_id')\n",
    "  result_col.remove('y')\n",
    "  test_size = 0.2\n",
    "  seed = 42\n",
    "  X_train, X_test, y_train, y_test = train_test_split(result[result_col], result['y'], test_size=test_size, random_state=seed)#, stratify = result['y'])\n",
    "  model = XGBClassifier(\n",
    "                base_score= 0.5, \n",
    "                booster= 'gbtree', \n",
    "                colsample_bylevel= 1, \n",
    "                colsample_bynode= 1, \n",
    "                colsample_bytree= 1, \n",
    "                gamma= 0, \n",
    "                learning_rate= 0.1,\n",
    "                max_delta_step= 0, \n",
    "                max_depth= 4, \n",
    "                min_child_weight= 1, \n",
    "#                 missing= None, \n",
    "                n_estimators= 100, \n",
    "                nthread= 1, \n",
    "                objective= 'binary:logistic', \n",
    "                reg_alpha= 0, \n",
    "                reg_lambda= 1, \n",
    "                scale_pos_weight= 1, \n",
    "                seed= 0, \n",
    "                subsample= 1,\n",
    "                verbosity= 1\n",
    "  \n",
    "  )\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_test)\n",
    "  predictions = [round(value) for value in y_pred]\n",
    "  # evaluate predictions\n",
    "  accuracy = accuracy_score(y_test, predictions)\n",
    "  print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "  f1 = f1_score(y_test, predictions)\n",
    "  print(\"F1 Scoure: %.2f%%\" % (f1 * 100.0))\n",
    "  print(precision_score(y_test, predictions))\n",
    "  print(recall_score(y_test, predictions))\n",
    "  return model, result_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "E41cMxVIjkaR"
   },
   "outputs": [],
   "source": [
    "def alert_output(alert, model_1, model_2, alert_col, result_col, doc):\n",
    "  data_db_cr_ratio = debit_credit_ratio_func(alert)\n",
    "  data_all_txn_cnt = all_txn_cnt(alert)\n",
    "\n",
    "  #Remit Submission\n",
    "  alert_data = alert\n",
    "  alert_data['proba'] = model_1.predict_proba(alert[alert_col])[:,1]\n",
    "  alert_result = alert_data[['cust_id','proba']]\n",
    "  alert_result['level'] = alert_result[\"proba\"].map(mapping)\n",
    "  alert_result = alert_result[['cust_id', 'level']]\n",
    "  alert_result['cnt'] = 1\n",
    "  alert_result = alert_result.groupby(['cust_id','level'])['cnt'].count().reset_index().pivot_table(index='cust_id', columns='level', values='cnt')\n",
    "  alert_result.fillna(0, inplace=True)\n",
    "  alert_result = alert_result.div(alert_result.sum(axis=1), axis=0)\n",
    "  alert_result = alert_result.reset_index()\n",
    "  for col in list(set([num for num in range(1,11,1)]) -  set(alert_result.columns[1:])):\n",
    "    alert_result[col] = 0.0\n",
    "  alert_result = alert_result.merge(data_db_cr_ratio, on='cust_id', how='left')\n",
    "  alert_result = alert_result.merge(data_all_txn_cnt, on='cust_id', how='left')\n",
    "    \n",
    "#   alert_result = alert_result.merge(da_features, on='cust_id', how='left')\n",
    "#   alert_result = alert_result.fillna(0)\n",
    "    \n",
    "  alert_pred = model_2.predict_proba(alert_result[result_col])\n",
    "  # evaluate predictions\n",
    "  alert_result = alert_result[['cust_id']]\n",
    "  alert_result['probability'] = alert_pred[:,1]\n",
    "\n",
    "  final = predict_alert_time.merge(custinfo[['alert_key', 'cust_id']].merge(alert_result, on='cust_id'), on='alert_key', how='left')[['alert_key', 'probability']]\n",
    "  doc = doc[['alert_key']]\n",
    "  final = doc.merge(final, on='alert_key', how='left')\n",
    "  final.fillna(0,inplace=True)\n",
    "  final.loc[final[final['alert_key'].isin(prev_list)].index,'probability']=0\n",
    "  return final, alert_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "FUdbYkG8o43l",
    "outputId": "9070f1df-dbd7-4e46-957e-23bcab0fe05b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1763: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.51%\n",
      "F1 Scoure: 52.87%\n",
      "0.3781230317027084\n",
      "0.8783223603999024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.63%\n",
      "F1 Scoure: 27.45%\n",
      "0.7\n",
      "0.17073170731707318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rocker\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alert_key</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>357307</td>\n",
       "      <td>0.009349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>376329</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>373644</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>357668</td>\n",
       "      <td>0.011804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>354443</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>364485</td>\n",
       "      <td>0.010302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3846</th>\n",
       "      <td>363155</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>368710</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>358067</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3849</th>\n",
       "      <td>372119</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3850 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      alert_key  probability\n",
       "0        357307     0.009349\n",
       "1        376329     0.000000\n",
       "2        373644     0.000000\n",
       "3        357668     0.011804\n",
       "4        354443     0.000000\n",
       "...         ...          ...\n",
       "3845     364485     0.010302\n",
       "3846     363155     0.000000\n",
       "3847     368710     0.000000\n",
       "3848     358067     0.000000\n",
       "3849     372119     0.000000\n",
       "\n",
       "[3850 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dp zone\n",
    "train_dp, alert_dp = train_alert_process_func(dp, custinfo, predict_alert_time)\n",
    "train_dp = train_prev_d(train_dp, 30)\n",
    "alert_dp = train_prev_d(alert_dp, 30)\n",
    "train_dp = preprocess(train_dp)\n",
    "alert_dp = preprocess(alert_dp)\n",
    "train_dp = dp_feature_func(train_dp)\n",
    "alert_dp = dp_feature_func(alert_dp)\n",
    "\n",
    "dp_col = ['cust_id','session_amt_diff_ratio', 'date_amt_diff_ratio',\n",
    "       'date_time_amt_diff_ratio','tx_cnt_date_time','tx_cnt_date',\n",
    "       'txbranch_day_time_cnt', 'txbranch_day_cnt', 'day_atm_txn_ratio',\n",
    "       'day_time_atm_txn_ratio', 'day_cross_bank_ratio', 'day_time_cross_bank_ratio', #'txbranch',\n",
    "       #'txbranch_session_cnt',#\n",
    "\n",
    "       'y']\n",
    "dp_X, dp_model_1, dp_feature_importance = model_training_1(train_dp, dp_col)\n",
    "dp_result = result_preprocess_func(train_dp, dp_X, dp_model_1)\n",
    "\n",
    "# dp_result = dp_result.merge(da_features, how='left')\n",
    "# dp_result = dp_result.fillna(0)\n",
    "\n",
    "\n",
    "dp_model_2, dp_result_col = model_training_2(dp_result)\n",
    "dp_col.remove('y')\n",
    "dp_col.remove('cust_id')\n",
    "dp_final, alert_dp_result = alert_output(alert_dp, dp_model_1, dp_model_2, dp_col, dp_result_col, doc)\n",
    "dp_final\n",
    "#F1 Scoure: 66.66%\n",
    "#0.9980544747081712\n",
    "#0.5003657644476956\n",
    "#F1 Scoure: 28.57%\n",
    "#0.875\n",
    "#0.17073170731707318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "cGvCK_odkTnw"
   },
   "outputs": [],
   "source": [
    "dp_final.to_csv('0.0497.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
